# copeer_lite.py
# –í–µ—Ä—Å–∏—è –±–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ TUI. –ü—Ä–æ—Å—Ç–æ–π, –Ω–∞–¥–µ–∂–Ω—ã–π –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥.

import argparse
import csv
import logging
import os
import re
import subprocess
import sys
import tarfile
import time
import yaml
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from threading import Lock, get_ident

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
# –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(level="INFO", format="[%(asctime)s] %(message)s", datefmt="%H:%M:%S")
log = logging.getLogger(__name__)


# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
__version__ = "1.0.0-lite"
CONFIG_FILE = "config.yaml"
DEFAULT_CONFIG = {
    'mount_points': ["/mnt/disk1", "/mnt/disk2"],
    'source_root': '/path/to/source/data',
    'destination_root': '/',
    'threshold': 98.0,
    'state_file': "copier_state.csv",
    'mapping_file': "mapping.csv",
    'error_log_file': "errors.log",
    'dry_run_mapping_file': "dry_run_mapping.csv",
    'dry_run': False,
    'threads': 8,
    'min_files_for_sequence': 50,
    'image_extensions': ['dpx', 'cri', 'tiff', 'tif', 'exr', 'png', 'jpg', 'jpeg', 'tga', 'j2c'],
}
SEQUENCE_RE = re.compile(r'^(.*?)[\._]*(\d+)\.([a-zA-Z0-9]+)$', re.IGNORECASE)
file_lock = Lock()


# --- –û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏ ---

class DiskManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—ã–±–æ—Ä–æ–º –¥–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ø–æ—Ä–æ–≥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è."""
    def __init__(self, mount_points, threshold):
        self.mount_points = mount_points
        self.threshold = threshold
        self.active_disk = None
        self.lock = Lock()
        self._select_initial_disk()

    def _get_disk_usage(self, path):
        if not os.path.exists(path): return 0.0
        try:
            st = os.statvfs(path)
            used = (st.f_blocks - st.f_bfree) * st.f_frsize
            total = st.f_blocks * st.f_frsize
            return round(used / total * 100, 2) if total > 0 else 0
        except FileNotFoundError: return 100

    def _select_initial_disk(self):
        for mount in self.mount_points:
            if not os.path.exists(mount):
                log.warning(f"–¢–æ—á–∫–∞ –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è {mount} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                continue
            if self._get_disk_usage(mount) < self.threshold:
                self.active_disk = mount
                log.info(f"–í—ã–±—Ä–∞–Ω –Ω–∞—á–∞–ª—å–Ω—ã–π –¥–∏—Å–∫: {self.active_disk}")
                return
        log.error("üõë –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∏—Å–∫–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.")
        raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∏—Å–∫–æ–≤")

    def get_current_destination(self):
        with self.lock:
            if not self.active_disk: raise RuntimeError("üõë –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–∫–æ–≤.")
            if self._get_disk_usage(self.active_disk) >= self.threshold:
                log.warning(f"–î–∏—Å–∫ {self.active_disk} –∑–∞–ø–æ–ª–Ω–µ–Ω. –ò—â—É —Å–ª–µ–¥—É—é—â–∏–π...")
                available_disks = [m for m in self.mount_points if os.path.exists(m)]
                try:
                    current_index = available_disks.index(self.active_disk)
                    next_disks = available_disks[current_index + 1:] + available_disks[:current_index]
                except (ValueError, IndexError):
                    next_disks = available_disks
                self.active_disk = next((m for m in next_disks if self._get_disk_usage(m) < self.threshold), None)
                if self.active_disk:
                    log.info(f"–ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –¥–∏—Å–∫: {self.active_disk}")
            if not self.active_disk:
                raise RuntimeError("üõë –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–∫–æ–≤: –≤—Å–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω—ã –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
            return self.active_disk

def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –µ–≥–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
    config = DEFAULT_CONFIG.copy()
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config.update(yaml.safe_load(f) or {})
        except Exception as e:
            log.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {CONFIG_FILE}: {e}. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
    else:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            yaml.dump(DEFAULT_CONFIG, f, sort_keys=False, allow_unicode=True)
        log.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {CONFIG_FILE}")
    config['image_extensions'] = set(e.lower() for e in config.get('image_extensions', []))
    return config

def load_previous_state(state_file):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª—é—á–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã."""
    processed = set()
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                processed.update(row[0] for row in csv.reader(f) if row)
            log.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed)} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è.")
        except Exception as e:
            log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_file}: {e}")
    return processed

def write_log(state_file, mapping_file, key, dest_path, is_dry_run):
    """–ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–ø–∏—Å—å –≤ –ª–æ–≥-—Ñ–∞–π–ª—ã."""
    with file_lock:
        if not is_dry_run:
            with open(state_file, "a", newline='', encoding='utf-8') as f:
                csv.writer(f).writerow([key])
        if dest_path:
            with open(mapping_file, "a", newline='', encoding='utf-8') as f:
                csv.writer(f).writerow([key, dest_path])

def find_sequences(dirs, config):
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–∞–ª–æ–≥–∞–º —Ñ–∞–π–ª–∞—Ö."""
    all_sequences, sequence_files = [], set()
    for dir_path, files_with_sizes in dirs.items():
        sequences_in_dir = defaultdict(list)
        for filename, file_size in files_with_sizes:
            match = SEQUENCE_RE.match(filename)
            if match and match.group(3).lower() in config.get('image_extensions', set()):
                prefix, frame, ext = match.groups()
                full_path = os.path.join(dir_path, filename)
                sequences_in_dir[(prefix, ext.lower())].append((int(frame), full_path, file_size))
        for (prefix, ext), file_tuples in sequences_in_dir.items():
            if len(file_tuples) >= config.get('min_files_for_sequence', 50):
                file_tuples.sort()
                frames, full_paths, sizes = zip(*file_tuples)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                min_frame, max_frame = min(frames), max(frames)
                expected_frames = max_frame - min_frame + 1
                actual_frames = len(frames)
                
                # –†–∞–∑—Ä–µ—à–∞–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–æ–ø—É—Å–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 5% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–æ–≤)
                max_allowed_gaps = max(1, int(expected_frames * 0.05))  # –ú–∞–∫—Å–∏–º—É–º 5% –ø—Ä–æ–ø—É—Å–∫–æ–≤
                missing_frames = expected_frames - actual_frames
                
                if missing_frames <= max_allowed_gaps:
                    # –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–µ–∫–≤–µ–Ω—Ü–∏—è —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
                    safe_prefix = re.sub(r'[^\w\.\-]', '_', prefix.strip())
                    tar_filename = f"{safe_prefix}.{min_frame:04d}-{max_frame:04d}.{ext}.tar"
                    virtual_tar_path = os.path.join(dir_path, tar_filename)
                    all_sequences.append({
                        'type': 'sequence', 
                        'key': virtual_tar_path, 
                        'dir_path': dir_path, 
                        'tar_filename': tar_filename, 
                        'source_files': list(full_paths), 
                        'size': sum(sizes),
                        'frame_info': {
                            'min_frame': min_frame,
                            'max_frame': max_frame,
                            'expected_frames': expected_frames,
                            'actual_frames': actual_frames,
                            'missing_frames': missing_frames
                        }
                    })
                    sequence_files.update(full_paths)
                else:
                    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ - –Ω–µ —Å—á–∏—Ç–∞–µ–º —Å–µ–∫–≤–µ–Ω—Ü–∏–µ–π
                    log.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ–∫–≤–µ–Ω—Ü–∏—è {prefix} –≤ {dir_path}: "
                              f"—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({missing_frames} –∏–∑ {expected_frames} –∫–∞–¥—Ä–æ–≤, "
                              f"–¥–æ–ø—É—Å—Ç–∏–º–æ –º–∞–∫—Å–∏–º—É–º {max_allowed_gaps})")
    return all_sequences, sequence_files

def archive_sequence_to_destination(job, dest_tar_path):
    """–°–æ–∑–¥–∞–µ—Ç tar-–∞—Ä—Ö–∏–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Å–µ–∫–≤–µ–Ω—Ü–∏–∏."""
    os.makedirs(os.path.dirname(dest_tar_path), exist_ok=True)
    with tarfile.open(dest_tar_path, "w") as tar:
        for file_path in job['source_files']:
            if os.path.exists(file_path):
                tar.add(file_path, arcname=os.path.basename(file_path))
            else:
                log.warning(f"–í —Å–µ–∫–≤–µ–Ω—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {file_path}")

# --- –õ–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ---

def analyze_and_plan_jobs(input_csv_path, config, processed_items_keys):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç CSV –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø–ª–∞–Ω —Ä–∞–±–æ—Ç."""
    log.info("--- –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ ---")
    log.info(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {input_csv_path}")

    parser_primary = re.compile(r'^"([^"]+)","([^"]+)",.*')
    parser_fallback = re.compile(r'^"([^"]+\.\w{2,5})",.*', re.IGNORECASE)

    dirs, all_files_from_csv = defaultdict(list), {}
    source_root = config.get('source_root')
    if source_root:
        log.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Ä–µ–Ω—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_root}")

    lines_total, lines_ignored_dirs, malformed_lines = 0, 0, []

    with open(input_csv_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            lines_total += 1
            cleaned_line = line.strip().replace('""', '"')
            if not cleaned_line: continue

            rel_path, file_type, size = None, "", 0
            match = parser_primary.match(cleaned_line)
            if match:
                rel_path, file_type = match.groups()
            else:
                match = parser_fallback.match(cleaned_line)
                if match: rel_path, file_type = match.group(1), "file"

            if rel_path:
                if 'directory' in file_type:
                    lines_ignored_dirs += 1
                    continue

                size_match = re.search(r',"(\d+)"$', cleaned_line)
                if size_match:
                    try: size = int(size_match.group(1))
                    except (ValueError, IndexError): size = 0

                absolute_source_path = os.path.normpath(os.path.join(source_root, rel_path) if source_root else rel_path)
                path_obj = Path(absolute_source_path)
                dirs[str(path_obj.parent)].append((path_obj.name, size))
                all_files_from_csv[absolute_source_path] = size
            else:
                malformed_lines.append((lines_total, line))

    sequences, sequence_files = find_sequences(dirs, config)
    standalone_files = set(all_files_from_csv.keys()) - sequence_files

    jobs = sequences + [{'type': 'file', 'key': f, 'size': all_files_from_csv[f]} for f in standalone_files]
    jobs_to_process = [job for job in jobs if job['key'] not in processed_items_keys]
    jobs_to_process.sort(key=lambda j: j.get('size', 0), reverse=True)

    # --- –í—ã–≤–æ–¥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ ---
    print("\n--- –û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É ---")
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ CSV —Ñ–∞–π–ª–µ:             {lines_total:,}")
    print(f"  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏):          {lines_ignored_dirs:,}")
    print(f"  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç): {len(malformed_lines):,}")
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:        {len(all_files_from_csv):,}")
    print("-" * 20)
    print(f"–ó–∞–¥–∞–Ω–∏–π –Ω–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏—é:                {len(sequences):,}")
    print(f"–ó–∞–¥–∞–Ω–∏–π –Ω–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ:              {len(standalone_files):,}")
    print("-" * 20)
    print(f"–í—Å–µ–≥–æ –∑–∞–¥–∞–Ω–∏–π –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é:          {len(jobs_to_process):,}")
    print(f"  –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã):       {len(jobs) - len(jobs_to_process):,}")
    print("-" * 20)
    if malformed_lines:
        print("\n[–í–ù–ò–ú–ê–ù–ò–ï] –ù–∞–π–¥–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ (–ø–µ—Ä–≤—ã–µ 10):")
        for num, line in malformed_lines[:10]:
            print(f"  –°—Ç—Ä–æ–∫–∞ #{num}: {line}")

    return jobs_to_process

def process_job_worker(job, config, disk_manager):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ –∑–∞–¥–∞–Ω–∏–µ, –ª–æ–≥–∏—Ä—É—è –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü."""
    thread_id = get_ident()
    is_dry_run = config['dry_run']
    short_name = job.get('tar_filename') or os.path.basename(job['key'])
    op_type = "–ê—Ä—Ö–∏–≤–∞—Ü–∏—è" if job['type'] == 'sequence' else "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ"

    log.info(f"[–ü–æ—Ç–æ–∫ {thread_id}] –ù–∞—á–∞–ª–æ: {op_type} -> {short_name}")

    try:
        dest_mount_point = disk_manager.get_current_destination()
        source_root = config.get('source_root')
        destination_root = config.get('destination_root', '/')
        absolute_source_key = job['key']

        if source_root and absolute_source_key.startswith(os.path.normpath(source_root) + os.sep):
            rel_path = os.path.relpath(absolute_source_key, source_root)
        else:
            rel_path = absolute_source_key.lstrip(os.path.sep)

        dest_path = os.path.normpath(os.path.join(dest_mount_point, destination_root.lstrip(os.path.sep), rel_path))

        source_keys_to_log = []
        if job['type'] == 'sequence':
            if not is_dry_run:
                archive_sequence_to_destination(job, dest_path)
            else:
                time.sleep(0.05)
            source_keys_to_log = job['source_files']
        else: # 'file'
            source_keys_to_log = [absolute_source_key]
            if not is_dry_run:
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                rsync_cmd = ["rsync", "-a", "--checksum", absolute_source_key, dest_path]
                subprocess.run(rsync_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            else:
                time.sleep(0.05)

        log.info(f"[–ü–æ—Ç–æ–∫ {thread_id}] –£—Å–ø–µ—Ö: {short_name}")
        return (job['type'], job['size'], source_keys_to_log, dest_path)

    except Exception as e:
        log.error(f"[–ü–æ—Ç–æ–∫ {thread_id}] –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {short_name}: {e}")
        with file_lock:
             with open(config['error_log_file'], "a", encoding='utf-8') as f:
                f.write(f"{time.asctime()};{job['key']};{e}\n")
        return (None, 0, None, None)

# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ ---

def main(args):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    config = load_config()
    if args.dry_run:
        config['dry_run'] = True

    log.info(f"--- Copeer v{__version__} ---")
    log.info(f"–†–µ–∂–∏–º: {'Dry Run' if config['dry_run'] else '–†–µ–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞'}")

    is_dry_run = config['dry_run']
    if is_dry_run:
        dry_run_log_path = config['dry_run_mapping_file']
        try:
            with open(dry_run_log_path, 'w', newline='', encoding='utf-8') as f:
                csv.writer(f).writerow(["source_path", "destination_path"])
            log.info(f"–û—Ç—á–µ—Ç dry-run –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {dry_run_log_path}")
        except IOError as e:
            log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –æ—Ç—á–µ—Ç–∞ –¥–ª—è dry-run: {e}")

    processed_items_keys = load_previous_state(config['state_file'])
    jobs_to_process = analyze_and_plan_jobs(args.input_file, config, processed_items_keys)
    if not jobs_to_process:
        log.info("–í—Å–µ –∑–∞–¥–∞–Ω–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
        return

    try:
        input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–ª–∏ Ctrl+C –¥–ª—è –æ—Ç–º–µ–Ω—ã...")
    except KeyboardInterrupt:
        log.warning("\n–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        return

    disk_manager = DiskManager(config['mount_points'], config['threshold'])

    log.info(f"--- –®–∞–≥ 2: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {len(jobs_to_process)} –∑–∞–¥–∞–Ω–∏–π ---")

    jobs_completed = 0
    total_jobs = len(jobs_to_process)

    with ThreadPoolExecutor(max_workers=config['threads']) as executor:
        future_to_job = {executor.submit(process_job_worker, job, config, disk_manager): job for job in jobs_to_process}

        for future in as_completed(future_to_job):
            job_type, _, source_keys, dest_path = future.result()

            jobs_completed += 1
            log.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {jobs_completed} / {total_jobs} –∑–∞–¥–∞–Ω–∏–π –≤—ã–ø–æ–ª–Ω–µ–Ω–æ.")

            if job_type:
                # –ó–∞–ø–∏—Å—å –≤ –ª–æ–≥-—Ñ–∞–π–ª—ã
                for key in source_keys:
                    write_log(
                        config['state_file'],
                        config['dry_run_mapping_file'] if is_dry_run else config['mapping_file'],
                        key, dest_path, is_dry_run
                    )

    log.info("--- –í—Å–µ –∑–∞–¥–∞–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã ---")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç CSV, –∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –∏ –∫–æ–ø–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ. Lite-–≤–µ—Ä—Å–∏—è –±–µ–∑ TUI.",
        prog="copeer_lite"
    )
    parser.add_argument(
        "-v", "--version",
        action="version",
        version=f"%(prog)s v{__version__}"
    )
    parser.add_argument("input_file", help="–ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.")
    parser.add_argument("--dry-run", action="store_true", help="–í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è.")
    args = parser.parse_args()
    main(args)
