# archiver_copier_v2.py
# –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–∫–µ—Ç–æ–≤ uv:
# 1. uv venv
# 2. source .venv/bin/activate (–∏–ª–∏ .\.venv\Scripts\activate –≤ Windows)
# 3. uv pip install -r requirements.txt

import os
import sys
import csv
import subprocess
import logging
import time
import yaml
import re
import tarfile
import argparse
from pathlib import Path
from rich.console import Console
from rich.progress import Progress, BarColumn, TextColumn, TaskProgressColumn, TransferSpeedColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.layout import Layout
from rich.live import Live
from rich.table import Table
from rich.logging import RichHandler
from rich.filesize import decimal
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, get_ident
from collections import defaultdict

console = Console()

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í–ï–†–°–ò–Ø ---
__version__ = "2.0.0"
# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ---
CONFIG_FILE = "config.yaml"
DEFAULT_CONFIG = {
    'mount_points': ["/mnt/disk1", "/mnt/disk2"],
    'source_base_path': None,
    'threshold': 98.0,
    'state_file': "copier_state.csv",
    'mapping_file': "mapping.csv",
    'error_log_file': "errors.log",
    'dry_run': False,
    'threads': 8,
    'min_files_for_sequence': 100,
    'image_extensions': ['dpx', 'tiff', 'tif', 'exr', 'png', 'jpg', 'jpeg', 'tga']
}
SEQUENCE_RE = re.compile(r'^(.*?)[\._]*(\d+)\.([a-zA-Z0-9]+)$', re.IGNORECASE)

logging.basicConfig(level="INFO", format="%(message)s", datefmt="[%X]", handlers=[RichHandler(rich_tracebacks=True, show_path=False, console=console)])
log = logging.getLogger("rich")

processed_items_keys = set()
file_lock = Lock()
worker_stats = defaultdict(lambda: {"status": "[grey50]–û–∂–∏–¥–∞–Ω–∏–µ...[/grey50]", "speed": 0})

# --- –ö–ª–∞—Å—Å—ã –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---

class DiskManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—ã–±–æ—Ä–æ–º –¥–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ø–æ—Ä–æ–≥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è."""
    def __init__(self, mount_points, threshold):
        self.mount_points = mount_points
        self.threshold = threshold
        self.active_disk = None
        self.lock = Lock()
        self._select_initial_disk()

    def _get_disk_usage(self, path):
        if not os.path.exists(path): return 0.0
        try:
            st = os.statvfs(path)
            used = (st.f_blocks - st.f_bfree) * st.f_frsize
            total = st.f_blocks * st.f_frsize
            return round(used / total * 100, 2) if total > 0 else 0
        except FileNotFoundError: return 100

    def _select_initial_disk(self):
        for mount in self.mount_points:
            if not os.path.exists(mount):
                log.warning(f"–¢–æ—á–∫–∞ –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è [bold yellow]{mount}[/bold yellow] –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                continue
            if self._get_disk_usage(mount) < self.threshold:
                self.active_disk = mount
                log.info(f"–í—ã–±—Ä–∞–Ω –Ω–∞—á–∞–ª—å–Ω—ã–π –¥–∏—Å–∫: [bold green]{self.active_disk}[/bold green]")
                return
        log.error("üõë –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∏—Å–∫–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.")

    def get_current_destination(self):
        with self.lock:
            if not self.active_disk: raise RuntimeError("üõë –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–∫–æ–≤.")
            if self._get_disk_usage(self.active_disk) >= self.threshold:
                log.warning(f"–î–∏—Å–∫ [bold]{self.active_disk}[/bold] –∑–∞–ø–æ–ª–Ω–µ–Ω. –ò—â—É —Å–ª–µ–¥—É—é—â–∏–π...")
                available_disks = [m for m in self.mount_points if os.path.exists(m)]
                try:
                    current_index = available_disks.index(self.active_disk)
                    next_disks = available_disks[current_index + 1:] + available_disks[:current_index]
                except (ValueError, IndexError): next_disks = available_disks
                self.active_disk = next((m for m in next_disks if self._get_disk_usage(m) < self.threshold), None)
                if self.active_disk: log.info(f"–ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –¥–∏—Å–∫: [bold green]{self.active_disk}[/bold green]")
            if not self.active_disk: raise RuntimeError("üõë –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–∫–æ–≤: –≤—Å–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω—ã –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
            return self.active_disk

    def get_all_disks_status(self):
        return [(m, self._get_disk_usage(m)) for m in self.mount_points]

def load_config():
    config = DEFAULT_CONFIG.copy()
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f: config.update(yaml.safe_load(f) or {})
        except Exception as e: console.print(f"[bold red]–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {CONFIG_FILE}: {e}. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.[/bold red]")
    else:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f: yaml.dump(DEFAULT_CONFIG, f, sort_keys=False, allow_unicode=True)
        log.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: [cyan]{CONFIG_FILE}[/cyan]")
    config['image_extensions'] = set(e.lower() for e in config.get('image_extensions', []))
    return config

def load_previous_state(state_file):
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r', encoding='utf-8') as f: processed_items_keys.update(row[0] for row in csv.reader(f) if row)
            log.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ [bold]{len(processed_items_keys)}[/bold] –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è.")
        except Exception as e: log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_file}: {e}")

def write_log(file_path, key, dest_path=None):
    with file_lock:
        with open(DEFAULT_CONFIG['state_file'], "a", newline='', encoding='utf-8') as f: csv.writer(f).writerow([key])
        if dest_path:
            with open(DEFAULT_CONFIG['mapping_file'], "a", newline='', encoding='utf-8') as f: csv.writer(f).writerow([key, dest_path])

def find_sequences(dirs, config):
    all_sequences, sequence_files = [], set()
    for dir_path, files_with_sizes in dirs.items():
        sequences_in_dir = defaultdict(list)
        for filename, file_size in files_with_sizes:
            match = SEQUENCE_RE.match(filename)
            if match and match.group(3).lower() in config['image_extensions']:
                prefix, frame, ext = match.groups()
                full_path = os.path.join(dir_path, filename)
                sequences_in_dir[(prefix, ext.lower())].append((int(frame), full_path, file_size))

        for (prefix, ext), file_tuples in sequences_in_dir.items():
            if len(file_tuples) >= config['min_files_for_sequence']:
                file_tuples.sort()
                frames, full_paths, sizes = zip(*file_tuples)
                min_frame, max_frame = min(frames), max(frames)

                # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ó–î–ï–°–¨ ---
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ –Ω–∞ –¥–≤–µ —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å UnboundLocalError
                safe_prefix = re.sub(r'[^\w\.\-]', '_', prefix.strip())
                tar_filename = f"{safe_prefix}.{min_frame:04d}-{max_frame:04d}.{ext}.tar"

                virtual_tar_path = os.path.join(dir_path, tar_filename)
                all_sequences.append({
                    'type': 'sequence',
                    'key': virtual_tar_path,
                    'dir_path': dir_path,
                    'tar_filename': tar_filename,
                    'source_files': list(full_paths),
                    'size': sum(sizes)
                })
                sequence_files.update(full_paths)
    return all_sequences, sequence_files

# --- –ò–ó–ú–ï–ù–ï–ù–û: –§—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –¥–ª—è TUI ---
def analyze_and_plan_jobs(input_csv_path, config):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç CSV, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø–ª–∞–Ω —Ä–∞–±–æ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É."""
    console.rule("[yellow]–®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ[/yellow]")
    log.info(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: [bold cyan]{input_csv_path}[/cyan]")

    line_parser = re.compile(r'^"(.*?)","(.*?)",.*,"(\d+)"$')
    dirs, all_files_from_csv = defaultdict(list), {}
    base_path = config.get('source_base_path')
    if base_path: log.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞: [cyan]{base_path}[/cyan]")
    try:
        with open(input_csv_path, 'r', encoding='utf-8', errors='ignore') as f:
            for i, line in enumerate(f):
                line = line.strip()
                if not line: continue
                match = line_parser.match(line)
                if not match: continue
                rel_path, file_type, size_str = match.groups()
                if 'regular file' not in file_type: continue
                try: size = int(size_str)
                except ValueError: continue
                full_path = os.path.normpath(os.path.join(base_path, rel_path) if base_path else rel_path)
                path_obj = Path(full_path)
                dirs[str(path_obj.parent)].append((path_obj.name, size))
                all_files_from_csv[full_path] = size
    except FileNotFoundError: console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: CSV-—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_csv_path}[/]"); sys.exit(1)
    except Exception as e: console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}[/]"); sys.exit(1)

    if not all_files_from_csv:
        log.warning("–í CSV –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
        sys.exit(0)

    sequences, sequence_files = find_sequences(dirs, config)
    standalone_files = set(all_files_from_csv.keys()) - sequence_files
    jobs = sequences + [{'type': 'file', 'key': f, 'size': all_files_from_csv[f]} for f in standalone_files]
    jobs_to_process = [job for job in jobs if job['key'] not in processed_items_keys]

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–¥–∞–Ω–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É, –æ—Ç —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –∫ —Å–∞–º–æ–º—É –º–∞–ª–µ–Ω—å–∫–æ–º—É.
    # –≠—Ç–æ —Å–º–µ—à–∞–µ—Ç —Å–µ–∫–≤–µ–Ω—Ü–∏–∏ –∏ —Ñ–∞–π–ª—ã –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–æ—Ç–æ–∫–æ–≤.
    jobs_to_process.sort(key=lambda j: j['size'], reverse=True)

    seq_jobs = [j for j in jobs_to_process if j['type'] == 'sequence']
    file_jobs = [j for j in jobs_to_process if j['type'] == 'file']

    # --- –ò–ó–ú–ï–ù–ï–ù–û: –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å —Å–æ —Å–≤–æ–¥–∫–æ–π –¥–ª—è TUI ---
    plan_summary = {
        "sequences": {"count": len(seq_jobs), "size": sum(j['size'] for j in seq_jobs)},
        "files": {"count": len(file_jobs), "size": sum(j['size'] for j in file_jobs)},
        "total": {"count": len(jobs_to_process), "size": sum(j['size'] for j in jobs_to_process)},
        "skipped": len(jobs) - len(jobs_to_process)
    }

    source_root = os.path.commonpath(list(all_files_from_csv.keys())) if all_files_from_csv else (base_path or os.getcwd())
    return jobs_to_process, plan_summary, source_root

def archive_sequence_to_destination(job, dest_tar_path):
    try:
        os.makedirs(os.path.dirname(dest_tar_path), exist_ok=True)
        with tarfile.open(dest_tar_path, "w") as tar:
            for file_path in job['source_files']:
                if os.path.exists(file_path): tar.add(file_path, arcname=os.path.basename(file_path))
                else: log.warning(f"–í —Å–µ–∫–≤–µ–Ω—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {file_path}")
        return True
    except Exception as e:
        log.error(f"‚úñ –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞—Ä—Ö–∏–≤–∞ {dest_tar_path}: {e}")
        if os.path.exists(dest_tar_path): os.remove(dest_tar_path)
        return False

def process_job_worker(job, source_root, config, disk_manager):
    thread_id, start_time = get_ident(), time.monotonic()
    try:
        dest_root = disk_manager.get_current_destination()
        try: rel_path = os.path.relpath(os.path.dirname(job['key']), start=source_root)
        except ValueError: rel_path = os.path.dirname(job['key']).lstrip(os.path.sep)

        if job['type'] == 'sequence':
            short_name = job['tar_filename']
            worker_stats[thread_id]['status'] = f"[yellow]–ê—Ä—Ö–∏–≤–∏—Ä—É—é:[/] {short_name}"
            dest_path = os.path.join(dest_root, rel_path, short_name)
            if not config['dry_run']:
                if not archive_sequence_to_destination(job, dest_path): raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤ {short_name}")
            else: time.sleep(0.1)
        else:
            short_name = os.path.basename(job['key'])
            worker_stats[thread_id]['status'] = f"[cyan]–ö–æ–ø–∏—Ä—É—é:[/] {short_name}"
            dest_path = os.path.join(dest_root, rel_path, short_name)
            if not config['dry_run']:
                if not os.path.exists(job['key']): raise FileNotFoundError(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {job['key']}")
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                subprocess.run(["rsync", "-a", "--checksum", job['key'], dest_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            else: time.sleep(0.01)

        if not config['dry_run']: write_log(config['state_file'], job['key'], dest_path)
        elapsed = time.monotonic() - start_time
        speed = job['size'] / elapsed if elapsed > 0 else 0
        worker_stats[thread_id]['status'] = "[green]–°–≤–æ–±–æ–¥–µ–Ω[/green]"
        worker_stats[thread_id]['speed'] = speed
        return job['key'], job['size'], job['type']
    except Exception as e:
        log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {job['key']}: {e}")
        short_name = os.path.basename(job['key'])
        worker_stats[thread_id]['status'] = f"[red]–û—à–∏–±–∫–∞:[/] {short_name}"
        worker_stats[thread_id]['speed'] = -1
        with open(config['error_log_file'], "a", encoding='utf-8') as f: f.write(f"{time.asctime()};{job['key']};{e}\n")
        return None, 0, job['type']

# --- –ò–ó–ú–ï–ù–ï–ù–û: –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è TUI —Ç–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞—é—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –º–∞–∫–µ—Ç ---
def make_layout() -> Layout:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É TUI."""
    layout = Layout(name="root")
    layout.split_column(
        Layout(name="top", ratio=1),
        Layout(name="middle", ratio=2),
        Layout(name="bottom", size=3) # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    )
    layout["top"].split_row(Layout(name="summary"), Layout(name="disks"))
    return layout

# --- –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞–Ω–µ–ª—å —Å–≤–æ–¥–∫–∏ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º ---
def generate_summary_panel(plan, completed) -> Panel:
    table = Table(box=None, expand=True)
    table.add_column("–¢–∏–ø –∑–∞–¥–∞–Ω–∏—è", style="cyan", no_wrap=True)
    table.add_column("–í—ã–ø–æ–ª–Ω–µ–Ω–æ (—à—Ç)", style="green", justify="right")
    table.add_column("–†–∞–∑–º–µ—Ä", style="green", justify="right")

    # –°–µ–∫–≤–µ–Ω—Ü–∏–∏
    s_done = completed['sequence']['count']
    s_total = plan['sequences']['count']
    s_size_done = completed['sequence']['size']
    s_size_total = plan['sequences']['size']
    table.add_row(
        "–ê—Ä—Ö–∏–≤–∞—Ü–∏—è —Å–µ–∫–≤–µ–Ω—Ü–∏–π",
        f"{s_done} / {s_total}",
        f"{decimal(s_size_done)} / {decimal(s_size_total)}"
    )
    # –§–∞–π–ª—ã
    f_done = completed['files']['count']
    f_total = plan['files']['count']
    f_size_done = completed['files']['size']
    f_size_total = plan['files']['size']
    table.add_row(
        "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤",
        f"{f_done} / {f_total}",
        f"{decimal(f_size_done)} / {decimal(f_size_total)}"
    )
    # –í—Å–µ–≥–æ
    t_done = s_done + f_done
    t_total = s_total + f_total
    t_size_done = s_size_done + f_size_done
    t_size_total = s_size_total + f_size_total
    table.add_row(
        "[bold]–í—Å–µ–≥–æ[/bold]",
        f"[bold]{t_done} / {t_total}[/bold]",
        f"[bold]{decimal(t_size_done)} / {decimal(t_size_total)}[/bold]"
    )
    return Panel(table, title="üìä –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", border_style="yellow")

def generate_disks_panel(disk_manager: DiskManager, config) -> Panel:
    table = Table(box=None, expand=True)
    table.add_column("–î–∏—Å–∫", style="white", no_wrap=True)
    table.add_column("–ó–∞–ø–æ–ª–Ω–µ–Ω–æ", style="green", ratio=1)
    table.add_column("%", style="bold", justify="right")
    for mount, percent in disk_manager.get_all_disks_status():
        color = "green" if percent < config['threshold'] else "red"
        bar = Progress(BarColumn(bar_width=None, style=color, complete_style=color), expand=True)
        task_id = bar.add_task("disk_usage", total=100); bar.update(task_id, completed=percent)
        is_active = " (*)" if mount == disk_manager.active_disk else ""
        table.add_row(f"[bold]{mount}{is_active}[/bold]", bar, f"{percent:.1f}%")
    return Panel(table, title="üì¶ –î–∏—Å–∫–∏", border_style="blue")

def generate_workers_panel(threads) -> Panel:
    table = Table(expand=True)
    table.add_column("–ü–æ—Ç–æ–∫", justify="center", style="cyan", width=12)
    table.add_column("–°—Ç–∞—Ç—É—Å", style="white", no_wrap=True, ratio=2)
    table.add_column("–°–∫–æ—Ä–æ—Å—Ç—å", justify="right", style="magenta", width=15)
    sorted_workers = sorted(worker_stats.keys())
    for tid in sorted_workers:
        stats, speed_str = worker_stats[tid], ""
        if stats['speed'] > 0: speed_str = f"{decimal(stats['speed'])}/s"
        elif stats['speed'] == -1: speed_str = "[red]ERROR[/red]"
        else: speed_str = "[dim]---[/dim]"
        table.add_row(str(tid), stats['status'], speed_str)
    return Panel(table, title=f"üë∑ –ü–æ—Ç–æ–∫–∏ ({threads})", border_style="green")

# --- –ò–ó–ú–ï–ù–ï–ù–û: –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å —É–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º TUI ---
def main(args):
    config = load_config()
    if args.dry_run: config['dry_run'] = True
    if args.source_base: config['source_base_path'] = args.source_base

    console.rule(f"[bold]Archiver & Copier V{__version__}[/bold] | –†–µ–∂–∏–º: {'Dry Run' if config['dry_run'] else '–†–µ–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞'}")

    load_previous_state(config['state_file'])

    jobs_to_process, plan_summary, source_root = analyze_and_plan_jobs(args.input_file, config)

    if not jobs_to_process:
        log.info("[bold green]‚úÖ –í—Å–µ –∑–∞–¥–∞–Ω–∏—è –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.[/bold green]")
        return
    if plan_summary['skipped'] > 0:
        log.info(f"[dim]–ü—Ä–æ–ø—É—â–µ–Ω–æ {plan_summary['skipped']} —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π.[/dim]")
    log.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω –æ–±—â–∏–π –∫–æ—Ä–µ–Ω—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è: [dim]{source_root}[/dim]")

    if not config['dry_run']:
        disk_manager = DiskManager(config['mount_points'], config['threshold'])
        if not disk_manager.active_disk: return
    else:
        class FakeDiskManager:
            active_disk = "/dry/run/dest"
            def get_current_destination(self): return self.active_disk
            def get_all_disks_status(self): return [(p, 0.0) for p in config['mount_points']]
        disk_manager = FakeDiskManager()
        log.info("Dry-run: —Å–∏–º—É–ª—è—Ü–∏—è –∑–∞–ø–∏—Å–∏ –Ω–∞ –¥–∏—Å–∫.")

    console.rule("[yellow]–®–∞–≥ 2: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ[/]")
    time.sleep(1)

    # --- –ò–ó–ú–ï–ù–ï–ù–û: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞ –∏ —Å—á–µ—Ç—á–∏–∫–æ–≤ –¥–ª—è TUI ---
    job_counter_column = TextColumn(f"[cyan]0/{plan_summary['total']['count']} –∑–∞–¥–∞–Ω–∏–π[/cyan]")
    progress = Progress(
        TextColumn("[bold blue]–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å:[/bold blue]"), BarColumn(), TaskProgressColumn(), TextColumn("‚Ä¢"),
        job_counter_column, TextColumn("‚Ä¢"), TransferSpeedColumn(), TextColumn("‚Ä¢"), TimeRemainingColumn()
    )
    main_task = progress.add_task("–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ", total=plan_summary['total']['size'])
    layout = make_layout()
    layout["bottom"].update(Panel(progress, title="üöÄ –ü—Ä–æ—Ü–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", border_style="magenta", expand=False))

    completed_stats = {
        "sequence": {"count": 0, "size": 0},
        "files": {"count": 0, "size": 0}
    }
    jobs_completed_count = 0
    all_jobs_successful = True

    try:
        with Live(layout, screen=True, redirect_stderr=False, vertical_overflow="visible") as live:
            with ThreadPoolExecutor(max_workers=config['threads']) as executor:
                # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–∞–Ω–µ–ª–µ–π
                layout["summary"].update(generate_summary_panel(plan_summary, completed_stats))
                layout["disks"].update(generate_disks_panel(disk_manager, config))
                layout["middle"].update(generate_workers_panel(config['threads']))

                future_to_job = {executor.submit(process_job_worker, job, source_root, config, disk_manager): job for job in jobs_to_process}

                for future in as_completed(future_to_job):
                    key, size_processed, job_type = future.result()

                    if key is not None:
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –ø–∞–Ω–µ–ª–∏ —Å–≤–æ–¥–∫–∏
                        if job_type == 'sequence':
                            completed_stats['sequence']['count'] += 1
                            completed_stats['sequence']['size'] += size_processed
                        else:
                            completed_stats['files']['count'] += 1
                            completed_stats['files']['size'] += size_processed
                    else:
                        all_jobs_successful = False

                    # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
                    jobs_completed_count += 1
                    progress.update(main_task, advance=size_processed)
                    job_counter_column.text_format = f"[cyan]{jobs_completed_count}/{plan_summary['total']['count']} –∑–∞–¥–∞–Ω–∏–π[/cyan]"

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–∞–Ω–µ–ª–µ–π TUI –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
                    layout["summary"].update(generate_summary_panel(plan_summary, completed_stats))
                    if not config['dry_run']:
                        layout["disks"].update(generate_disks_panel(disk_manager, config))
                    layout["middle"].update(generate_workers_panel(config['threads']))

    except (Exception, KeyboardInterrupt):
        console.print_exception(show_locals=False)
        console.print("\n[bold red]–ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω. –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ.[/bold red]")
        sys.exit(1)

    if all_jobs_successful and progress.finished:
        console.rule("[bold green]‚úÖ –í—Å–µ –∑–∞–¥–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã[/bold green]")
    else:
        console.rule("[bold yellow]–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –Ω–æ –±—ã–ª–∏ –æ—à–∏–±–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥.[/bold yellow]")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç CSV, –∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –∏ –∫–æ–ø–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.")
    parser.add_argument(
        "-v", "--version",
        action="version",
        version=f"%(prog)s v{__version__}"
    )
    parser.add_argument("input_file", help="–ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.")
    parser.add_argument("--dry-run", action="store_true", help="–í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è.")
    parser.add_argument("--source-base", help="–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π –∏–∑ CSV.")
    args = parser.parse_args()
    main(args)
