# copeer_rich_TUI.py
"""
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
—Å TUI-–¥–∞—à–±–æ—Ä–¥–æ–º –Ω–∞ –±–∞–∑–µ Rich –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã.
"""

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
import argparse, csv, logging, os, re, subprocess, sys, tarfile, time, yaml, math
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from pathlib import Path
from threading import Lock

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from rich.console import Console
from rich.filesize import decimal
from rich.layout import Layout
from rich.live import Live
from rich.logging import RichHandler
from rich.panel import Panel
from rich.progress import (BarColumn, Progress, TaskProgressColumn,
                           TextColumn, TimeRemainingColumn, TransferSpeedColumn)
from rich.prompt import Prompt
from rich.table import Table

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
console = Console()
__version__ = "5.0.1" # –ò–°–ü–†–ê–í–õ–ï–ù–û
CONFIG_FILE = "config.yaml"
status_queue = Queue()
DEFAULT_CONFIG = {
    'mount_points': ["/mnt/disk1", "/mnt/disk2"],
    'source_root': '/path/to/source/data',
    'destination_root': '/',
    'threshold': 98.0,
    'state_file': "copier_state.csv",
    'mapping_file': "mapping.csv",
    'error_log_file': "errors.log",
    'dry_run_mapping_file': "dry_run_mapping.csv",
    'threads': 8,
    'disk_strategy': "round_robin", #or fill
    'max_concurrent_disks': "2",
    'min_files_for_sequence': 50,
    'image_extensions': ['dpx', 'cri', 'tiff', 'tif', 'exr', 'png', 'jpg', 'jpeg', 'tga', 'j2c'],
}
SEQUENCE_RE = re.compile(r'^(.*?)[\._]*(\d+)\.([a-zA-Z0-9]+)$', re.IGNORECASE)

logging.basicConfig(level="INFO", format="%(message)s", datefmt="[%X]", handlers=[RichHandler(rich_tracebacks=True, show_path=False, console=console)])
log = logging.getLogger("rich")

file_lock = Lock()
worker_stats = {}

# --- –û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å—ã ---

# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç–æ—Ç –∫–ª–∞—Å—Å —Ü–µ–ª–∏–∫–æ–º
class DiskManager:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—ã–±–æ—Ä–æ–º –¥–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—É–ª –¥–∏—Å–∫–æ–≤,
    –Ω–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏—â–µ—Ç –º–µ—Å—Ç–æ –≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Å–±–æ–µ–≤.
    """
    def __init__(self, mount_points, threshold, strategy='fill', is_dry_run=False, max_concurrent_disks=999):
        self.mount_points = mount_points
        self.threshold = threshold
        self.strategy = strategy
        self.is_dry_run = is_dry_run
        self.max_concurrent_disks = int(max_concurrent_disks)
        self.active_disk = None
        self.next_disk_index = 0
        self.lock = Lock()

        log.info(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –¥–∏—Å–∫–∞–º: [bold cyan]{self.strategy}[/bold cyan]")
        if self.strategy == 'round_robin':
            log.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–∏—Å–∫–æ–≤ (–≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ): [bold cyan]{self.max_concurrent_disks}[/bold cyan]")
        self._select_initial_disk()

    def _get_disk_usage(self, path):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö."""
        if self.is_dry_run: return 0.0
        if not os.path.exists(path): return 100.0
        try:
            st = os.statvfs(path)
            used = (st.f_blocks - st.f_bfree) * st.f_frsize
            total = st.f_blocks * st.f_frsize
            return round(used / total * 100, 2) if total > 0 else 0.0
        except FileNotFoundError: return 100.0

    def _get_disk_free_space(self, path):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ –≤ –±–∞–π—Ç–∞—Ö."""
        if self.is_dry_run: return sys.maxsize
        if not os.path.exists(path): return 0
        try:
            st = os.statvfs(path)
            return st.f_bavail * st.f_frsize
        except FileNotFoundError:
            return 0

    def _is_disk_suitable(self, mount_path, required_space=0):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –¥–∏—Å–∫ –ø–æ –≤—Å–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, –µ—Å—Ç—å –º–µ—Å—Ç–æ)."""
        if not self.is_dry_run:
            if not os.path.exists(mount_path):
                return False
            if self._get_disk_usage(mount_path) >= self.threshold:
                return False
            if self._get_disk_free_space(mount_path) <= required_space:
                return False
        return True

    def _select_initial_disk(self):
        for mount in self.mount_points:
            if self._is_disk_suitable(mount):
                self.active_disk = mount
                log.info(f"–ù–∞–π–¥–µ–Ω –∫–∞–∫ –º–∏–Ω–∏–º—É–º –æ–¥–∏–Ω –¥–æ—Å—Ç—É–ø–Ω—ã–π –¥–∏—Å–∫ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã: [bold green]{self.active_disk}[/bold green]")
                return
        log.error("üõë –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∏—Å–∫–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.")
        raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∏—Å–∫–æ–≤")

    # –ü–û–õ–ù–û–°–¢–¨–Æ –ü–ï–†–ï–†–ê–ë–û–¢–ê–ù–ù–´–ô –ú–ï–¢–û–î –° –î–í–£–•–§–ê–ó–ù–´–ú –ü–û–ò–°–ö–û–ú
    def get_current_destination(self, job_size=0):
        with self.lock:
            if self.strategy == 'round_robin':
                # --- –§–ê–ó–ê 1: –ü–û–ü–´–¢–ö–ê –ù–ê–ô–¢–ò –ú–ï–°–¢–û –í –ü–†–ò–û–†–ò–¢–ï–¢–ù–û–ú –ü–£–õ–ï ---
                preferred_pool = self.mount_points[:self.max_concurrent_disks]

                if preferred_pool:
                    for i in range(len(preferred_pool)):
                        check_index = (self.next_disk_index + i) % len(preferred_pool)
                        disk_to_check = preferred_pool[check_index]

                        if self._is_disk_suitable(disk_to_check, job_size):
                            # –ù–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–∏—Å–∫ –≤ –ø—É–ª–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                            self.next_disk_index = (check_index + 1) % len(preferred_pool)
                            return disk_to_check

                # --- –§–ê–ó–ê 2: –ê–í–ê–†–ò–ô–ù–´–ô –ü–û–ò–°–ö –í –û–°–¢–ê–õ–¨–ù–´–• –î–ò–°–ö–ê–• ---
                log.warning(f"[yellow]–í –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–º –ø—É–ª–µ ({self.max_concurrent_disks} —à—Ç.) –Ω–µ—Ç –º–µ—Å—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {decimal(job_size)}. –ò—â—É –≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–∏—Å–∫–∞—Ö...[/yellow]")

                fallback_disks = self.mount_points[self.max_concurrent_disks:]
                for disk in fallback_disks:
                    if self._is_disk_suitable(disk, job_size):
                        log.info(f"–ù–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–∏—Å–∫ –≤–Ω–µ –ø—É–ª–∞: [bold green]{disk}[/bold green]")
                        # –í–∞–∂–Ω–æ: –Ω–µ –º–µ–Ω—è–µ–º self.next_disk_index, —Ç.–∫. –º—ã —Ä–∞–±–æ—Ç–∞–µ–º –≤–Ω–µ –ø—É–ª–∞
                        return disk

                # –ï—Å–ª–∏ –∏ –∑–¥–µ—Å—å –Ω–µ –Ω–∞—à–ª–∏, –∑–Ω–∞—á–∏—Ç –º–µ—Å—Ç–∞ –Ω–µ—Ç –Ω–∏–≥–¥–µ
                raise RuntimeError(f"üõë –í–æ –í–°–ï–• –¥–∏—Å–∫–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –º–µ—Å—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–∞ —Ä–∞–∑–º–µ—Ä–æ–º {decimal(job_size)}")

            else:  # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 'fill'
                suitable_disks = [m for m in self.mount_points if self._is_disk_suitable(m, job_size)]
                if not suitable_disks:
                    raise RuntimeError(f"üõë –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–∫–æ–≤ –¥–ª—è —Ñ–∞–π–ª–∞ —Ä–∞–∑–º–µ—Ä–æ–º {decimal(job_size)}.")

                if self.active_disk not in suitable_disks:
                    log.warning(f"–î–∏—Å–∫ [bold]{self.active_disk}[/bold] –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º {decimal(job_size)}. –ò—â—É —Å–ª–µ–¥—É—é—â–∏–π...")
                    self.active_disk = suitable_disks[0]
                    log.info(f"–ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –¥–∏—Å–∫: [bold green]{self.active_disk}[/bold green]")
                return self.active_disk

    def get_all_disks_status(self):
        return [(m, self._get_disk_usage(m)) for m in self.mount_points]
# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def load_config():
    config = DEFAULT_CONFIG.copy()
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f: config.update(yaml.safe_load(f) or {})
        except Exception as e: console.print(f"[bold red]–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {CONFIG_FILE}: {e}.[/bold red]")
    else:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f: yaml.dump(DEFAULT_CONFIG, f, sort_keys=False, allow_unicode=True)
        log.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: [cyan]{CONFIG_FILE}[/cyan]")
    config['image_extensions'] = set(e.lower() for e in config.get('image_extensions', []))
    return config

def load_previous_state(state_file, processed_items_keys):
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                for row in csv.reader(f):
                    if row: processed_items_keys.add(row[0])
            log.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ [bold]{len(processed_items_keys)}[/bold] –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è.")
        except Exception as e: log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_file}: {e}")

def write_log(state_log_file, mapping_log_file, key, dest_path=None, is_dry_run=False):
    with file_lock:
        if not is_dry_run:
            with open(state_log_file, "a", newline='', encoding='utf-8') as f: csv.writer(f).writerow([key])
        if dest_path:
            with open(mapping_log_file, "a", newline='', encoding='utf-8') as f: csv.writer(f).writerow([key, dest_path])

def find_sequences(dirs, config):
    all_sequences, sequence_files = [], set()
    for dir_path, files_with_sizes in dirs.items():
        sequences_in_dir = defaultdict(list)
        for filename, file_size in files_with_sizes:
            match = SEQUENCE_RE.match(filename)
            if match and match.group(3).lower() in config.get('image_extensions', set()):
                prefix, frame, ext = match.groups()
                sequences_in_dir[(prefix, ext.lower())].append((int(frame), os.path.join(dir_path, filename), file_size))
        for (prefix, ext), file_tuples in sequences_in_dir.items():
            if len(file_tuples) >= config.get('min_files_for_sequence', 50):
                file_tuples.sort()
                frames, full_paths, sizes = zip(*file_tuples)
                min_frame, max_frame = min(frames), max(frames)
                safe_prefix = re.sub(r'[^\w\.\-]', '_', prefix.strip())
                tar_filename = f"{safe_prefix}.{min_frame:04d}-{max_frame:04d}.{ext}.tar"
                virtual_tar_path = os.path.join(dir_path, tar_filename)
                all_sequences.append({'type': 'sequence', 'key': virtual_tar_path, 'dir_path': dir_path, 'tar_filename': tar_filename, 'source_files': list(full_paths), 'size': sum(sizes)})
                sequence_files.update(full_paths)
    return all_sequences, sequence_files

def archive_sequence_to_destination(job, dest_tar_path, progress_callback=None):
    try:
        os.makedirs(os.path.dirname(dest_tar_path), exist_ok=True)
        source_files = job.get('source_files', [])
        total_files = len(source_files)
        with tarfile.open(dest_tar_path, "w") as tar:
            for i, file_path in enumerate(source_files):
                if os.path.exists(file_path):
                    tar.add(file_path, arcname=os.path.basename(file_path))
                else:
                    log.warning(f"–í —Å–µ–∫–≤–µ–Ω—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {file_path}")
                if progress_callback:
                    progress_callback(i + 1, total_files)
        return True
    except (IOError, OSError, tarfile.TarError) as e:
        log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å –∞—Ä—Ö–∏–≤ {dest_tar_path}: {e}")
        return False

def parse_scientific_notation(size_str: str) -> int:
    try:
        cleaned_str = size_str.replace(',', '.').strip()
        if 'E' in cleaned_str.upper(): return int(float(cleaned_str))
        return int(cleaned_str)
    except (ValueError, TypeError): return 0

# --- –õ–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ---

def scan_directory_and_plan_jobs(source_dir_path, config, processed_items_keys):
    console.rule("[yellow]–®–∞–≥ 1: –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ[/]")
    console.print(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞: [bold cyan]{source_dir_path}[/bold cyan]")
    all_file_paths = []
    with console.status("[bold green]–ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤...[/bold green]"):
        for root, _, files in os.walk(source_dir_path):
            for name in files:
                all_file_paths.append(os.path.join(root, name))

    dirs, all_files_map = defaultdict(list), {}
    with Progress(console=console, transient=True) as progress:
        task = progress.add_task("[green]–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤...", total=len(all_file_paths))
        for path in all_file_paths:
            try:
                size = os.path.getsize(path)
                path_obj = Path(path)
                dirs[str(path_obj.parent)].append((path_obj.name, size))
                all_files_map[str(path_obj)] = size
            except FileNotFoundError:
                log.warning(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤–æ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {path}")
            progress.update(task, advance=1)

    sequences, sequence_files = find_sequences(dirs, config)
    standalone_files = set(all_files_map.keys()) - sequence_files
    archive_jobs = [job for job in sequences if job['key'] not in processed_items_keys]
    copy_jobs = [{'type': 'file', 'key': f, 'size': all_files_map[f]} for f in standalone_files if f not in processed_items_keys]
    archive_jobs.sort(key=lambda j: j.get('size', 0), reverse=True)
    copy_jobs.sort(key=lambda j: j.get('size', 0), reverse=True)
    stats = {"total_found": len(all_files_map), "mode": "dir"}
    return copy_jobs, archive_jobs, stats

def analyze_and_plan_jobs(input_csv_path, config, processed_items_keys):
    console.rule("[yellow]–®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ[/]")
    console.print(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: [bold cyan]{input_csv_path}[/bold cyan]")
    dirs, all_files_from_csv = defaultdict(list), {}
    source_root = config.get('source_root')
    if source_root: console.print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Ä–µ–Ω—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞: [cyan]{source_root}[/cyan]")
    lines_total, lines_ignored_dirs, malformed_lines = 0, 0, []

    try:
        with open(input_csv_path, 'r', encoding='utf-8', errors='ignore') as f: total_lines_for_progress = sum(1 for _ in f)
        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[green]–ê–Ω–∞–ª–∏–∑ CSV...", total=total_lines_for_progress)
            with open(input_csv_path, 'r', encoding='utf-8', errors='ignore') as f:
                reader = csv.reader(f, delimiter=';')
                for i, row in enumerate(reader):
                    progress.update(task, advance=1); lines_total += 1
                    if not row or len(row) < 5: malformed_lines.append((i + 1, str(row), "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–ª–æ–Ω–æ–∫")); continue
                    rel_path, file_type, size_str = row[0], row[1], row[4]
                    if 'directory' in file_type: lines_ignored_dirs += 1; continue
                    if 'file' not in file_type: malformed_lines.append((i + 1, str(row), f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø: {file_type}")); continue
                    size = parse_scientific_notation(size_str)
                    absolute_source_path = os.path.normpath(os.path.join(source_root, rel_path) if source_root else rel_path)
                    path_obj = Path(absolute_source_path)
                    dirs[str(path_obj.parent)].append((path_obj.name, size))
                    all_files_from_csv[absolute_source_path] = size
    except Exception as e: console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}[/bold red]"); sys.exit(1)

    sequences, sequence_files = find_sequences(dirs, config)
    standalone_files = set(all_files_from_csv.keys()) - sequence_files
    archive_jobs_all = sequences
    copy_jobs_all = [{'type': 'file', 'key': f, 'size': all_files_from_csv[f]} for f in standalone_files]
    archive_jobs_to_process = [job for job in archive_jobs_all if job['key'] not in processed_items_keys]
    copy_jobs_to_process = [job for job in copy_jobs_all if job['key'] not in processed_items_keys]
    archive_jobs_to_process.sort(key=lambda j: j.get('size', 0), reverse=True)
    copy_jobs_to_process.sort(key=lambda j: j.get('size', 0), reverse=True)
    stats = {
        "mode": "csv", "lines_total": lines_total, "lines_ignored_dirs": lines_ignored_dirs,
        "malformed_lines": malformed_lines, "total_found": len(all_files_from_csv)
    }
    return copy_jobs_to_process, archive_jobs_to_process, stats

# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–ª–∏–∫–æ–º
def show_summary_and_confirm(copy_jobs, archive_jobs, stats):
    while True:
        console.rule("[yellow]–û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É[/]")
        report_table = Table(title=None, show_header=False, box=None, padding=(0, 2))
        report_table.add_column("–ü–∞—Ä–∞–º–µ—Ç—Ä", style="cyan", no_wrap=True)
        report_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="white", justify="right")

        # --- –ë–ª–æ–∫ 1: –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (CSV –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏) ---
        if stats.get("mode") == "csv":
            report_table.add_row("–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ CSV:", f"{stats.get('lines_total', 0):,}")
            malformed_count = len(stats.get('malformed_lines', []))
            # –ù–æ–≤–∞—è, –±–æ–ª–µ–µ –ø–æ–Ω—è—Ç–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞
            report_table.add_row("  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç):", f"[{'red' if malformed_count > 0 else 'dim'}]{malformed_count:,}[/]")
            report_table.add_row("  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–∑–∞–ø–∏—Å–∏ —Å —Ç–∏–ø–æ–º 'directory'):", f"[dim]{stats.get('lines_ignored_dirs', 0):,}[/dim]")
            report_table.add_section()

        # --- –ë–ª–æ–∫ 2: –°–≤–æ–¥–∫–∞ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º –∏ –∏—Ö —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å—É ---
        total_valid_files = stats.get('total_found', 0)
        report_table.add_row("[bold]–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:", f"[bold green]{total_valid_files:,}[/bold green]")
        report_table.add_section()

        # –¢–µ–ø–µ—Ä—å —Ä–∞—Å—á–µ—Ç—ã –∏ –≤—ã–≤–æ–¥ —Å—Ç–∞–ª–∏ –ª–æ–≥–∏—á–Ω—ã–º–∏
        archive_size = sum(j.get('size', 0) for j in archive_jobs)
        copy_size = sum(j.get('size', 0) for j in copy_jobs)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        processed_count = total_valid_files - (len(copy_jobs) + len(archive_jobs))

        report_table.add_row("–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ (–ø—Ä–æ–ø—É—â–µ–Ω–æ):", f"[dim]{processed_count:,}[/dim]")
        report_table.add_row("–ù–æ–≤—ã—Ö –∑–∞–¥–∞–Ω–∏–π –Ω–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏—é:", f"[yellow]{len(archive_jobs):,}[/yellow] ({decimal(archive_size)})")
        report_table.add_row("–ù–æ–≤—ã—Ö –∑–∞–¥–∞–Ω–∏–π –Ω–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ:", f"[yellow]{len(copy_jobs):,}[/yellow] ({decimal(copy_size)})")

        console.print(report_table)

        choices = ["s", "q"]
        prompt_text = "\n[bold]–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: ([green]S[/green])—Ç–∞—Ä—Ç / ([red]Q[/red])uit"
        malformed_lines = stats.get('malformed_lines', [])
        if malformed_lines:
            choices.append("e")
            prompt_text += " / ([yellow]E[/yellow])rrors"
        choice = Prompt.ask(prompt_text, choices=choices, default="s").lower()
        if choice == 's': return True
        if choice == 'q': return False
        if choice == 'e' and malformed_lines:
            console.print("\n[bold yellow]----- –°–ø–∏—Å–æ–∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å—Ç—Ä–æ–∫ (–ø–µ—Ä–≤—ã–µ 50) -----[/bold yellow]")
            for num, line, reason in malformed_lines[:50]: console.print(f"[dim]–°—Ç—Ä–æ–∫–∞ #{num} ({reason}):[/dim] {line}")
            console.input("\n[bold]–ù–∞–∂–º–∏—Ç–µ [green]Enter[/green] –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é...[/bold]")
            console.clear()
# –ò–°–ü–†–ê–í–õ–ï–ù–û: –Ø–≤–Ω–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç is_dry_run
# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–ª–∏–∫–æ–º
# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–ª–∏–∫–æ–º
def process_job_worker(worker_id, job, config, disk_manager, is_dry_run, is_debug_mode, progress_callback=None):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞–Ω–∏–µ, –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ –æ—á–µ—Ä–µ–¥—å –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ª–æ–≥–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏
    –∫–∞–∫ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, —Ç–∞–∫ –∏ –¥–ª—è —Å–µ–∫–≤–µ–Ω—Ü–∏–π.
    """
    short_name = job.get('tar_filename') or os.path.basename(job['key'])
    op_type_text = "[yellow]–ê—Ä—Ö–∏–≤–∞—Ü–∏—è[/yellow]" if job['type'] == 'sequence' else "[cyan]–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ[/cyan]"
    status_queue.put((worker_id, {"status": op_type_text, "job": job, "progress": 0, "disk_idx": None}))

    try:
        dest_mount_point = disk_manager.get_current_destination(job['size'])

        disk_idx = '?'
        try:
            disk_idx = config['mount_points'].index(dest_mount_point) + 1
        except (ValueError, KeyError):
            pass

        status_queue.put((worker_id, {"disk_idx": disk_idx}))

        source_root = config.get('source_root')
        destination_root = config.get('destination_root', '/')
        absolute_source_key = job['key']

        rel_path = os.path.relpath(absolute_source_key, source_root) if source_root and absolute_source_key.startswith(source_root) else absolute_source_key.lstrip(os.path.sep)
        dest_path = os.path.normpath(os.path.join(dest_mount_point, destination_root.lstrip(os.path.sep), rel_path))

        source_keys_to_log = []

        if job['type'] == 'sequence':
            # –î–ª—è —Å–µ–∫–≤–µ–Ω—Ü–∏–π –≤ –ª–æ–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ–π–¥—É—Ç –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
            source_keys_to_log = job.get('source_files', [])
            if not is_dry_run:
                if not archive_sequence_to_destination(job, dest_path, progress_callback):
                    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤ {short_name}")
            else: # Dry-run —Å–∏–º—É–ª—è—Ü–∏—è
                total_files = len(job.get('source_files', []))
                for i in range(total_files):
                    time.sleep(0.005)
                    if progress_callback: progress_callback(i + 1, total_files)
        else:  # 'file'
            # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ - —Ç–æ–ª—å–∫–æ –µ–≥–æ –∫–ª—é—á
            source_keys_to_log = [absolute_source_key]
            if not is_dry_run:
                if not os.path.exists(absolute_source_key): raise FileNotFoundError(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {absolute_source_key}")
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)

                # ... (–±–ª–æ–∫ rsync –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
                rsync_cmd = ["rsync", "-a", "--no-i-r", "--progress", absolute_source_key, dest_path]
                status_queue.put((worker_id, {"status": "[blue]rsync...[/blue]"}))
                process = subprocess.Popen(rsync_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8', errors='ignore')
                progress_re = re.compile(r'\s+(\d+)%')
                line_buffer = ""
                for char in iter(lambda: process.stdout.read(1), ''):
                    if char in ['\r', '\n']:
                        if match := progress_re.search(line_buffer):
                            status_queue.put((worker_id, {"progress": int(match.group(1))}))
                        line_buffer = ""
                    else:
                        line_buffer += char
                process.stdout.close()
                return_code = process.wait()
                if return_code != 0 and "died with <Signals.SIGINT: 2>" not in (error_output := process.stderr.read()) and return_code != -2:
                    raise subprocess.CalledProcessError(return_code, rsync_cmd, stderr=error_output)

            else: # Dry-run —Å–∏–º—É–ª—è—Ü–∏—è
                steps = 5 if is_debug_mode else 3
                delay = 0.7 if is_debug_mode else 0.2
                for i in range(steps):
                    status_queue.put((worker_id, {"status": "[cyan]–°–∏–º—É–ª—è—Ü–∏—è[/cyan]", "progress": f"{i+1}/{steps}"}))
                    time.sleep(delay)

        final_status = "[bold green]–ó–∞–≤–µ—Ä—à–µ–Ω–æ[/bold green]"
        if is_dry_run: final_status = "[green]–ì–æ—Ç–æ–≤–æ (dr)[/green]"
        status_queue.put((worker_id, {"status": final_status, "progress": 100}))

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∏–ø, —Ä–∞–∑–º–µ—Ä –∏ –ö–õ–Æ–ß–ò –ò–°–•–û–î–ù–´–• –§–ê–ô–õ–û–í –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ª–æ–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        return (job['type'], job['size'], source_keys_to_log, dest_path)

    except Exception as e:
        if not isinstance(e, KeyboardInterrupt):
            status_queue.put((worker_id, {"status": "[bold red]–û—à–∏–±–∫–∞[/bold red]", "progress": 0}))

            # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï –í –õ–û–ì–ò–†–û–í–ê–ù–ò–ò –û–®–ò–ë–û–ö ---
            if job['type'] == 'sequence':
                # –ï—Å–ª–∏ —É–ø–∞–ª–∞ —Å–µ–∫–≤–µ–Ω—Ü–∏—è, –ª–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –µ—ë –ò–°–•–û–î–ù–´–ï —Ñ–∞–π–ª—ã
                log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–µ–∫–≤–µ–Ω—Ü–∏–∏ {job['key']}: {e}")
                error_message = f"Sequence processing failed for '{job['key']}': {e}"
                with file_lock:
                    with open(config['error_log_file'], "a", encoding='utf-8') as f:
                        for source_file in job.get('source_files', []):
                             f.write(f"{time.asctime()};{source_file};{error_message}\n")
            else:
                # –ï—Å–ª–∏ —É–ø–∞–ª –æ–±—ã—á–Ω—ã–π —Ñ–∞–π–ª, –ª–æ–≥–∏—Ä—É–µ–º –µ–≥–æ –∫–ª—é—á, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ
                log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {job['key']}: {e}")
                with file_lock:
                    with open(config['error_log_file'], "a", encoding='utf-8') as f:
                        f.write(f"{time.asctime()};{job['key']};{e}\n")

        return (None, 0, None, None)

def make_layout() -> Layout:
    layout = Layout(name="root")
    layout.split_column(Layout(name="top", size=19), Layout(name="middle"), Layout(name="bottom", size=3))
    layout["top"].split_row(Layout(name="summary"), Layout(name="disks"))
    return layout

def generate_summary_panel(plan, completed) -> Panel:
    table = Table(box=None, expand=True)
    table.add_column("–¢–∏–ø –∑–∞–¥–∞–Ω–∏—è", style="cyan", no_wrap=True)
    table.add_column("–í—ã–ø–æ–ª–Ω–µ–Ω–æ", style="green", justify="right")
    table.add_column("–†–∞–∑–º–µ—Ä", style="green", justify="right")
    s_plan, f_plan = plan.get('sequences', {}), plan.get('files', {})
    s_done, s_total = completed['sequence']['count'], s_plan.get('count', 0)
    s_size_done, s_size_total = completed['sequence']['size'], s_plan.get('size', 0)
    table.add_row("–ê—Ä—Ö–∏–≤–∞—Ü–∏—è", f"{s_done} / {s_total}", f"{decimal(s_size_done)} / {decimal(s_size_total)}")
    f_done, f_total = completed['files']['count'], f_plan.get('count', 0)
    f_size_done, f_size_total = completed['files']['size'], f_plan.get('size', 0)
    table.add_row("–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ", f"{f_done} / {f_total}", f"{decimal(f_size_done)} / {decimal(f_size_total)}")
    total_count_done, total_count_plan = s_done + f_done, s_total + f_total
    total_size_done, total_size_plan = s_size_done + f_size_done, s_size_total + f_size_total
    table.add_row("[bold]–í—Å–µ–≥–æ[/bold]", f"[bold]{total_count_done} / {total_count_plan}[/bold]", f"[bold]{decimal(total_size_done)} / {decimal(total_size_plan)}[/bold]")
    return Panel(table, title="üìä –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", border_style="yellow")

def generate_disks_panel(disk_manager: DiskManager, config) -> Panel:
    table = Table(box=None, expand=True)
    table.add_column("–î–∏—Å–∫", style="white", no_wrap=True)
    table.add_column("–†–∞–∑–º–µ—Ä", style="dim", justify="right")
    table.add_column("–ó–∞–ø–æ–ª–Ω–µ–Ω–æ", style="green", ratio=1)
    table.add_column("%", style="bold", justify="right")
    for mount, percent in disk_manager.get_all_disks_status():
        color = "green" if percent < config['threshold'] else "red"
        try:
            st = os.statvfs(mount); total = st.f_blocks * st.f_frsize; used = total - (st.f_bfree * st.f_frsize)
            size_str = f"{decimal(used)} / {decimal(total)}"
        except FileNotFoundError: size_str = "[red]–ù/–î[/red]"
        bar = Progress(BarColumn(bar_width=None, style=color, complete_style=color))
        bar.add_task("d", total=100, completed=percent)
        is_active = " (*)" if mount == disk_manager.active_disk else ""
        table.add_row(f"[bold]{mount}{is_active}[/bold]", size_str, bar, f"{percent:.1f}%")
    return Panel(table, title="üì¶ –î–∏—Å–∫–∏", border_style="blue")

# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–ª–∏–∫–æ–º
def generate_workers_panel(threads) -> Panel:
    table = Table(box=None, expand=True, show_header=True)
    table.add_column("–†–∞–∑–º–µ—Ä", justify="right", style="cyan", width=12)
    table.add_column("–ò–º—è —Ñ–∞–π–ª–∞", style="white", no_wrap=True, ratio=2)
    table.add_column("–°—Ç–∞—Ç—É—Å", justify="left", style="white", width=20) # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–º —à–∏—Ä–∏–Ω—É
    table.add_column("–ü—Ä–æ–≥—Ä–µ—Å—Å", justify="left", ratio=2)

    for worker_id in range(1, threads + 1):
        stats = worker_stats.get(worker_id)
        if stats and (job := stats.get("job")):
            size_str = decimal(job['size'])
            short_name = job.get('tar_filename') or os.path.basename(job['key'])
            status_text = stats.get("status", "")
            progress_val = stats.get("progress", 0)

            # --- –ù–û–í–´–ô –ë–õ–û–ö: –§–û–†–ú–ò–†–£–ï–ú –°–¢–†–û–ö–£ –°–¢–ê–¢–£–°–ê –° –î–ò–°–ö–û–ú ---
            disk_idx = stats.get("disk_idx")
            if disk_idx:
                status_with_disk = f"{status_text} [dim]‚ûú [[/dim][bold cyan]{disk_idx}[/bold cyan][dim]][/dim]"
            else:
                status_with_disk = status_text
            # -----------------------------------------------------

            progress_widget = ""
            if isinstance(progress_val, (int, float)) and progress_val > 0:
                progress_widget = Progress(BarColumn(bar_width=None), TaskProgressColumn(), expand=True)
                progress_widget.add_task("p", total=100, completed=progress_val)
            elif progress_val:
                progress_widget = str(progress_val)

            table.add_row(size_str, short_name, status_with_disk, progress_widget)
        elif stats:
            table.add_row("[dim]---[/dim]", f"[grey50]{stats.get('status', '–û–∂–∏–¥–∞–Ω–∏–µ...')}[/grey50]", "", "")

    return Panel(table, title=f"üë∑ –ü–æ—Ç–æ–∫–∏ ({threads})", border_style="green")
# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ ---

def main(args):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."""
    config = load_config()
    if args.dry_run: config['dry_run'] = True

    # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥–∏ –æ–¥–∏–Ω —Ä–∞–∑ –≤ –Ω–∞—á–∞–ª–µ
    is_dry_run = config['dry_run']
    is_debug_mode = os.getenv("COPEER_DEBUG") == "1"

    if is_debug_mode and is_dry_run:
        console.rule("[bold yellow]‚ö†Ô∏è  –ê–ö–¢–ò–í–ï–ù –†–ï–ñ–ò–ú –û–¢–õ–ê–î–ö–ò DRY-RUN ‚ö†Ô∏è[/bold yellow]")
        console.print("–°–∏–º—É–ª—è—Ü–∏—è —Ä–∞–±–æ—Ç—ã –±—É–¥–µ—Ç –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ–π –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏.")

    console.rule(f"[bold]Copeer v{__version__}[/bold] | –î–≤—É—Ö—Ñ–∞–∑–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")

    if is_dry_run:
        dry_run_log_path = config['dry_run_mapping_file']
        try:
            with open(dry_run_log_path, 'w', newline='', encoding='utf-8') as f:
                csv.writer(f).writerow(["source_path", "destination_path"])
        except IOError as e: console.print(f"[bold red]–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –æ—Ç—á–µ—Ç–∞: {e}[/bold red]")

    processed_items_keys = set()
    load_previous_state(config['state_file'], processed_items_keys)

    if args.input_file:
        copy_jobs, archive_jobs, stats = analyze_and_plan_jobs(args.input_file, config, processed_items_keys)
    elif args.source_dir:
        config['source_root'] = os.path.abspath(args.source_dir)
        copy_jobs, archive_jobs, stats = scan_directory_and_plan_jobs(args.source_dir, config, processed_items_keys)
    else:
        console.print("[bold red]–û—à–∏–±–∫–∞: –ù–µ —É–∫–∞–∑–∞–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö (--input-file –∏–ª–∏ --source-dir).[/bold red]"); sys.exit(1)

    if not copy_jobs and not archive_jobs:
        console.print("[green]–í—Å–µ –∑–∞–¥–∞–Ω–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.[/green]"); return
    # --- –ù–û–í–´–ô –ë–õ–û–ö: –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ó–ê–î–ê–ù–ò–ô –í –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –û–¢ –†–ï–ñ–ò–ú–ê ---
    if args.mode == 'copy':
        console.print("[bold cyan]–†–µ–∂–∏–º '—Ç–æ–ª—å–∫–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ': –∑–∞–¥–∞–Ω–∏—è –Ω–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏—é –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã.[/bold cyan]")
        archive_jobs = []
    elif args.mode == 'archive':
        console.print("[bold yellow]–†–µ–∂–∏–º '—Ç–æ–ª—å–∫–æ –∞—Ä—Ö–∏–≤–∞—Ü–∏—è': –∑–∞–¥–∞–Ω–∏—è –Ω–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã.[/bold yellow]")
        copy_jobs = []
    # --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê ---
    # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—Ç—á–µ—Ç –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤—Å–µ–≥–¥–∞ –≤—ã–∑—ã–≤–∞—é—Ç—Å—è
    if not show_summary_and_confirm(copy_jobs, archive_jobs, stats):
        console.print("[yellow]–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.[/yellow]"); sys.exit(0)

# –°–æ–∑–¥–∞–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π DiskManager –≤—Å–µ–≥–¥–∞, –Ω–æ –ø–µ—Ä–µ–¥–∞–µ–º –µ–º—É —Ñ–ª–∞–≥ is_dry_run
    disk_manager = DiskManager(
        config['mount_points'],
        config['threshold'],
        config.get('disk_strategy', 'fill'),
        is_dry_run=is_dry_run,
        max_concurrent_disks=config.get('max_concurrent_disks', 999) # 999 - "–±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç—å" –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    )
    if not is_dry_run and not disk_manager.active_disk: return

    for i in range(1, config['threads'] + 1):
        worker_stats[i] = {"status": "[grey50]–û–∂–∏–¥–∞–Ω–∏–µ...[/grey50]", "job": None, "progress": 0}

    layout = make_layout()
    completed_stats = {"sequence": {"count": 0, "size": 0}, "files": {"count": 0, "size": 0}}
    plan_summary = {
        "sequences": {"count": len(archive_jobs), "size": sum(j.get('size', 0) for j in archive_jobs)},
        "files": {"count": len(copy_jobs), "size": sum(j.get('size', 0) for j in copy_jobs)}
    }

    layout["summary"].update(generate_summary_panel(plan_summary, completed_stats))
    layout["disks"].update(generate_disks_panel(disk_manager, config))
    layout["middle"].update(generate_workers_panel(config['threads']))

    executor = None
    try:
        with Live(layout, screen=True, redirect_stderr=False, vertical_overflow="visible", refresh_per_second=10) as live:
            with ThreadPoolExecutor(max_workers=config['threads']) as executor:
                if copy_jobs:
                    progress_bar = Progress(TextColumn("[bold blue]–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ:[/bold blue]"), BarColumn(), TaskProgressColumn(), "‚Ä¢", TransferSpeedColumn())
                    main_task = progress_bar.add_task("copying", total=sum(j.get('size', 0) for j in copy_jobs))
                    layout["bottom"].update(Panel(progress_bar, title="üöÄ –§–∞–∑–∞ 1: –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ", border_style="magenta", height=3))

                    worker_id_queue, id_lock = list(range(1, config['threads'] + 1)), Lock()
                    def get_worker_id():
                        with id_lock: return worker_id_queue.pop(0) if worker_id_queue else None
                    def release_worker_id(worker_id):
                        if worker_id is not None:
                            with id_lock:
                                worker_id_queue.append(worker_id)

                    def job_wrapper(job):
                        worker_id = None
                        while worker_id is None:
                            worker_id = get_worker_id()
                            if worker_id is None: time.sleep(0.1)
                        try:
                            # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –Ø–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ —Ñ–ª–∞–≥–æ–≤
                            return process_job_worker(worker_id, job, config, disk_manager, is_dry_run, is_debug_mode)
                        finally: release_worker_id(worker_id)

                    active_futures = {executor.submit(job_wrapper, job) for job in copy_jobs}

                    while active_futures:
                        while not status_queue.empty():
                            worker_id, update_data = status_queue.get(); worker_stats[worker_id].update(update_data)

                        done_futures = {f for f in active_futures if f.done()}
                        for future in done_futures:
                            job_type, size, keys, path = future.result()
                            if job_type:
                                for key in keys: write_log(config['state_file'], config['mapping_file'], key, path, is_dry_run)
                                completed_stats['files']['count'] += 1; completed_stats['files']['size'] += size
                                progress_bar.update(main_task, advance=size)
                        active_futures -= done_futures

                        layout["summary"].update(generate_summary_panel(plan_summary, completed_stats))
                        layout["disks"].update(generate_disks_panel(disk_manager, config))
                        layout["middle"].update(generate_workers_panel(config['threads']))
                        time.sleep(0.1)

                if archive_jobs:
                    for i in range(1, config['threads'] + 1):
                        worker_stats[i] = {"status": "[grey50]–û–∂–∏–¥–∞–Ω–∏–µ...[/grey50]", "job": None, "progress": 0}

                    progress_bar = Progress(TextColumn("[bold yellow]–ê—Ä—Ö–∏–≤–∞—Ü–∏—è:[/bold yellow]"), BarColumn(), TaskProgressColumn(), "‚Ä¢", TimeRemainingColumn())
                    main_task = progress_bar.add_task("archiving", total=len(archive_jobs))
                    layout["bottom"].update(Panel(progress_bar, title="üì¶ –§–∞–∑–∞ 2: –ê—Ä—Ö–∏–≤–∞—Ü–∏—è (1 –ø–æ—Ç–æ–∫)", border_style="yellow", height=3))

                    for job in archive_jobs:
                        def progress_callback(current, total):
                            status_queue.put((1, {"progress": (current / total) * 100}))

                        future = executor.submit(process_job_worker, 1, job, config, disk_manager, is_dry_run, is_debug_mode, progress_callback)

                        while not future.done():
                            while not status_queue.empty():
                                worker_id, update_data = status_queue.get(); worker_stats[worker_id].update(update_data)
                            layout["summary"].update(generate_summary_panel(plan_summary, completed_stats))
                            layout["disks"].update(generate_disks_panel(disk_manager, config))
                            layout["middle"].update(generate_workers_panel(config['threads']))
                            time.sleep(0.1)

                        job_type, size, keys, path = future.result()
                        if job_type:
                            for key in keys: write_log(config['state_file'], config['mapping_file'], key, path, is_dry_run)
                            completed_stats['sequence']['count'] += 1; completed_stats['sequence']['size'] += size
                        progress_bar.update(main_task, advance=1)

    except KeyboardInterrupt:
        console.print("\n[bold yellow]–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ... –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤...[/bold yellow]")
    finally:
        if 'executor' in locals() and executor:
            executor.shutdown(wait=True, cancel_futures=True)
        console.print("\n[bold green]–í—ã—Ö–æ–¥.[/bold green]")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫, –∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –∏ –∫–æ–ø–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã —Å TUI-–¥–∞—à–±–æ—Ä–¥–æ–º.", prog="copeer")
    parser.add_argument("-v", "--version", action="version", version=f"%(prog)s v{__version__}")
    source_group = parser.add_mutually_exclusive_group(required=True)
    source_group.add_argument("-i", "--input-file", help="–ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.")
    source_group.add_argument("-s", "--source-dir", help="–ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
    parser.add_argument("--dry-run", action="store_true", help="–í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è.")
    parser.add_argument("--mode", choices=['all', 'copy', 'archive'], default='all', help="–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: 'all' - –≤—Å—ë (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), 'copy' - —Ç–æ–ª—å–∫–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ, 'archive' - —Ç–æ–ª—å–∫–æ –∞—Ä—Ö–∏–≤–∞—Ü–∏—è.")
    args = parser.parse_args()
    main(args)
