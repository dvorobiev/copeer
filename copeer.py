# copeer.py
"""
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
—Å TUI-–¥–∞—à–±–æ—Ä–¥–æ–º –Ω–∞ –±–∞–∑–µ Rich –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã.

–û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ê–Ω–∞–ª–∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ CSV-—Ñ–∞–π–ª–∞ —Å –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–º —Ñ–∞–π–ª–æ–≤.
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–æ–≤—ã—Ö —Å–µ–∫–≤–µ–Ω—Ü–∏–π –∏ –∏—Ö –∞—Ä—Ö–∏–≤–∞—Ü–∏—è "–Ω–∞ –ª–µ—Ç—É".
- –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞—Ä—Ö–∏–≤–∞—Ü–∏—è.
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π TUI —Å –∂–∏–≤—ã–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è rsync.
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏—Å–∫–æ–≤ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–æ—Ä–æ–≥–∞.
- –ù–∞–¥–µ–∂–Ω–æ–µ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ—Å–ª–µ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è.
- –ì–∏–±–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —á–µ—Ä–µ–∑ config.yaml.
"""

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
import argparse
import csv
import fcntl  # Unix-only
import logging
import os
import re
import select  # Unix-only
import subprocess
import sys
import tarfile
import time
import yaml
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from threading import Lock, get_ident

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from rich.console import Console
from rich.filesize import decimal
from rich.layout import Layout
from rich.live import Live
from rich.logging import RichHandler
from rich.panel import Panel
from rich.progress import (BarColumn, Progress, TaskProgressColumn,
                           TextColumn, TimeRemainingColumn, TransferSpeedColumn)
from rich.prompt import Prompt
from rich.table import Table

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
console = Console()
__version__ = "2.4.0"
CONFIG_FILE = "config.yaml"
DEFAULT_CONFIG = {
    'mount_points': ["/mnt/disk1", "/mnt/disk2"],
    'source_root': '/path/to/source/data',
    'destination_root': '/',
    'threshold': 98.0,
    'state_file': "copier_state.csv",
    'mapping_file': "mapping.csv",
    'error_log_file': "errors.log",
    'dry_run_mapping_file': "dry_run_mapping.csv",
    'dry_run': False,
    'progress_mode': 'simple',
    'threads': 8,
    'min_files_for_sequence': 50,
    'image_extensions': ['dpx', 'cri', 'tiff', 'tif', 'exr', 'png', 'jpg', 'jpeg', 'tga', 'j2c']
}
SEQUENCE_RE = re.compile(r'^(.*?)[\._]*(\d+)\.([a-zA-Z0-9]+)$', re.IGNORECASE)

logging.basicConfig(level="INFO", format="%(message)s", datefmt="[%X]", handlers=[RichHandler(rich_tracebacks=True, show_path=False, console=console)])
log = logging.getLogger("rich")

processed_items_keys = set()
file_lock = Lock()
worker_stats = defaultdict(lambda: {"status": "[grey50]–û–∂–∏–¥–∞–Ω–∏–µ...[/grey50]", "speed": "", "progress": None})


# --- –û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å—ã ---

class DiskManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—ã–±–æ—Ä–æ–º –¥–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ø–æ—Ä–æ–≥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è."""
    def __init__(self, mount_points, threshold):
        self.mount_points = mount_points
        self.threshold = threshold
        self.active_disk = None
        self.lock = Lock()
        self._select_initial_disk()

    def _get_disk_usage(self, path):
        if not os.path.exists(path): return 0.0
        try:
            st = os.statvfs(path)
            used = (st.f_blocks - st.f_bfree) * st.f_frsize
            total = st.f_blocks * st.f_frsize
            return round(used / total * 100, 2) if total > 0 else 0
        except FileNotFoundError: return 100

    def _select_initial_disk(self):
        for mount in self.mount_points:
            if not os.path.exists(mount):
                log.warning(f"–¢–æ—á–∫–∞ –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è [bold yellow]{mount}[/bold yellow] –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                continue
            if self._get_disk_usage(mount) < self.threshold:
                self.active_disk = mount
                log.info(f"–í—ã–±—Ä–∞–Ω –Ω–∞—á–∞–ª—å–Ω—ã–π –¥–∏—Å–∫: [bold green]{self.active_disk}[/bold green]")
                return
        log.error("üõë –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∏—Å–∫–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.")

    def get_current_destination(self):
        with self.lock:
            if not self.active_disk: raise RuntimeError("üõë –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–∫–æ–≤.")
            if self._get_disk_usage(self.active_disk) >= self.threshold:
                log.warning(f"–î–∏—Å–∫ [bold]{self.active_disk}[/bold] –∑–∞–ø–æ–ª–Ω–µ–Ω. –ò—â—É —Å–ª–µ–¥—É—é—â–∏–π...")
                available_disks = [m for m in self.mount_points if os.path.exists(m)]
                try:
                    current_index = available_disks.index(self.active_disk)
                    next_disks = available_disks[current_index + 1:] + available_disks[:current_index]
                except (ValueError, IndexError): next_disks = available_disks
                self.active_disk = next((m for m in next_disks if self._get_disk_usage(m) < self.threshold), None)
                if self.active_disk: log.info(f"–ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –¥–∏—Å–∫: [bold green]{self.active_disk}[/bold green]")
            if not self.active_disk: raise RuntimeError("üõë –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–∫–æ–≤: –≤—Å–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω—ã –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
            return self.active_disk

    def get_all_disks_status(self):
        return [(m, self._get_disk_usage(m)) for m in self.mount_points]


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –µ–≥–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
    config = DEFAULT_CONFIG.copy()
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f: config.update(yaml.safe_load(f) or {})
        except Exception as e: console.print(f"[bold red]–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {CONFIG_FILE}: {e}. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.[/bold red]")
    else:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f: yaml.dump(DEFAULT_CONFIG, f, sort_keys=False, allow_unicode=True)
        log.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: [cyan]{CONFIG_FILE}[/cyan]")
    config['image_extensions'] = set(e.lower() for e in config.get('image_extensions', []))
    return config

def load_previous_state(state_file):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª—é—á–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã."""
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r', encoding='utf-8') as f: processed_items_keys.update(row[0] for row in csv.reader(f) if row)
            log.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ [bold]{len(processed_items_keys)}[/bold] –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è.")
        except Exception as e: log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_file}: {e}")

def write_log(state_log_file, mapping_log_file, key, dest_path=None, is_dry_run=False):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ state –∏ mapping —Ñ–∞–π–ª—ã."""
    with file_lock:
        if not is_dry_run:
            with open(state_log_file, "a", newline='', encoding='utf-8') as f: csv.writer(f).writerow([key])
        if dest_path:
            with open(mapping_log_file, "a", newline='', encoding='utf-8') as f: csv.writer(f).writerow([key, dest_path])

def find_sequences(dirs, config):
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–∞–ª–æ–≥–∞–º —Ñ–∞–π–ª–∞—Ö."""
    all_sequences, sequence_files = [], set()
    for dir_path, files_with_sizes in dirs.items():
        sequences_in_dir = defaultdict(list)
        for filename, file_size in files_with_sizes:
            match = SEQUENCE_RE.match(filename)
            if match and match.group(3).lower() in config['image_extensions']:
                prefix, frame, ext = match.groups()
                full_path = os.path.join(dir_path, filename)
                sequences_in_dir[(prefix, ext.lower())].append((int(frame), full_path, file_size))
        for (prefix, ext), file_tuples in sequences_in_dir.items():
            if len(file_tuples) >= config['min_files_for_sequence']:
                file_tuples.sort()
                frames, full_paths, sizes = zip(*file_tuples)
                min_frame, max_frame = min(frames), max(frames)
                safe_prefix = re.sub(r'[^\w\.\-]', '_', prefix.strip())
                tar_filename = f"{safe_prefix}.{min_frame:04d}-{max_frame:04d}.{ext}.tar"
                virtual_tar_path = os.path.join(dir_path, tar_filename)
                all_sequences.append({'type': 'sequence', 'key': virtual_tar_path, 'dir_path': dir_path, 'tar_filename': tar_filename, 'source_files': list(full_paths), 'size': sum(sizes)})
                sequence_files.update(full_paths)
    return all_sequences, sequence_files

def archive_sequence_to_destination(job, dest_tar_path):
    """–°–æ–∑–¥–∞–µ—Ç tar-–∞—Ä—Ö–∏–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Å–µ–∫–≤–µ–Ω—Ü–∏–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ —Ü–µ–ª–µ–≤–æ–º –∫–∞—Ç–∞–ª–æ–≥–µ."""
    try:
        os.makedirs(os.path.dirname(dest_tar_path), exist_ok=True)
        with tarfile.open(dest_tar_path, "w") as tar:
            for file_path in job['source_files']:
                if os.path.exists(file_path): tar.add(file_path, arcname=os.path.basename(file_path))
                else: log.warning(f"–í —Å–µ–∫–≤–µ–Ω—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {file_path}")
        return True
    except Exception as e:
        log.error(f"‚úñ –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞—Ä—Ö–∏–≤–∞ {dest_tar_path}: {e}")
        if os.path.exists(dest_tar_path): os.remove(dest_tar_path)
        return False


# --- –õ–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ---

def analyze_and_plan_jobs(input_csv_path, config):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç CSV, –∏—Å–ø–æ–ª—å–∑—É—è –∏–µ—Ä–∞—Ä—Ö–∏—é –ø–∞—Ä—Å–µ—Ä–æ–≤, –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç –∏ –∂–¥–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è.
    """
    console.rule("[yellow]–®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ[/]")
    console.print(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: [bold cyan]{input_csv_path}[/bold cyan]")

    # –ü–∞—Ä—Å–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫
    parser_primary = re.compile(r'^"([^"]+)","([^"]+)",.*')
    parser_fallback = re.compile(r'^"([^"]+\.\w{2,5})",.*', re.IGNORECASE)

    dirs, all_files_from_csv = defaultdict(list), {}
    source_root = config.get('source_root')
    if source_root:
        console.print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Ä–µ–Ω—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞: [cyan]{source_root}[/cyan]")

    lines_total, lines_ignored_dirs, malformed_lines = 0, 0, []

    try:
        with open(input_csv_path, 'r', encoding='utf-8', errors='ignore') as f:
            total_lines_for_progress = sum(1 for _ in f)

        with Progress(console=console) as progress:
            task = progress.add_task("[green]–ê–Ω–∞–ª–∏–∑ CSV...", total=total_lines_for_progress)
            with open(input_csv_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    progress.update(task, advance=1)
                    lines_total += 1
                    cleaned_line = line.strip().replace('""', '"')
                    if not cleaned_line:
                        malformed_lines.append((lines_total, line, "–ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞"))
                        continue

                    rel_path, file_type, size = None, "", 0
                    match = parser_primary.match(cleaned_line)
                    if match:
                        rel_path, file_type = match.groups()
                    else:
                        match = parser_fallback.match(cleaned_line)
                        if match:
                            rel_path, file_type = match.group(1), "file"

                    if rel_path:
                        if 'directory' in file_type:
                            lines_ignored_dirs += 1
                            continue

                        size_match = re.search(r',"(\d+)"$', cleaned_line)
                        if size_match:
                            try: size = int(size_match.group(1))
                            except (ValueError, IndexError): size = 0

                        absolute_source_path = os.path.normpath(os.path.join(source_root, rel_path) if source_root else rel_path)
                        path_obj = Path(absolute_source_path)
                        dirs[str(path_obj.parent)].append((path_obj.name, size))
                        all_files_from_csv[absolute_source_path] = size
                    else:
                        malformed_lines.append((lines_total, line, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫–∏"))

    except (KeyboardInterrupt, SystemExit):
        console.print("\n[yellow]–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.[/yellow]")
        sys.exit(0)
    except FileNotFoundError: console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: CSV-—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_csv_path}[/]"); sys.exit(1)
    except Exception as e: console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}[/]"); sys.exit(1)

    if not all_files_from_csv:
        # ... –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –æ—Ç—á–µ—Ç—ã ...
        if malformed_lines:
            console.print("\n[bold red]–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:[/bold red]")
            for num, err_line, reason in malformed_lines[:50]: console.print(f"[dim]–°—Ç—Ä–æ–∫–∞ #{num} ({reason}):[/dim] {err_line}")
        else:
            log.warning("–í CSV –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        sys.exit(0)

    sequences, sequence_files = find_sequences(dirs, config)
    standalone_files = set(all_files_from_csv.keys()) - sequence_files

    jobs = sequences + [{'type': 'file', 'key': f, 'size': all_files_from_csv[f]} for f in standalone_files]
    jobs_to_process = [job for job in jobs if job['key'] not in processed_items_keys]

    if not jobs_to_process:
        log.info("[bold green]‚úÖ –í—Å–µ –∑–∞–¥–∞–Ω–∏—è –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.[/bold green]")
        return [], None

    jobs_to_process.sort(key=lambda j: j.get('size', 0), reverse=True)
    seq_jobs = [j for j in jobs_to_process if j['type'] == 'sequence']
    file_jobs = [j for j in jobs_to_process if j['type'] == 'file']

    # –¶–∏–∫–ª —Å –º–µ–Ω—é –≤—ã–±–æ—Ä–∞
    while True:
        console.rule("[yellow]–û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É[/]")
        report_table = Table(title=None, show_header=False, box=None, padding=(0, 2))
        report_table.add_column("–ü–∞—Ä–∞–º–µ—Ç—Ä", style="cyan", no_wrap=True)
        report_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="white", justify="right")

        report_table.add_row("–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ CSV —Ñ–∞–π–ª–µ:", f"{lines_total:,}")
        report_table.add_row("  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏):", f"[dim]{lines_ignored_dirs:,}[/dim]")
        malformed_count = len(malformed_lines)
        report_table.add_row(f"  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç):", f"[{'red' if malformed_count > 0 else 'dim'}]{malformed_count:,}[/]")
        report_table.add_row("[bold]–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:", f"[bold green]{len(all_files_from_csv):,}[/bold green]")
        report_table.add_section()
        report_table.add_row("–ò–∑ –Ω–∏—Ö —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–æ –≤ —Å–µ–∫–≤–µ–Ω—Ü–∏–∏:", f"{len(sequence_files):,}")
        report_table.add_row("  –ß—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–¥–∞–Ω–∏—è–º –Ω–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏—é:", f"[yellow]{len(sequences):,}[/yellow]")
        report_table.add_row("–û—Å—Ç–∞–ª–æ—Å—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è:", f"{len(standalone_files):,}")
        report_table.add_section()
        report_table.add_row("–í—Å–µ–≥–æ –∑–∞–¥–∞–Ω–∏–π –¥–æ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:", f"{len(jobs):,}")
        report_table.add_row("  –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã):", f"[dim]{len(jobs) - len(jobs_to_process):,}[/dim]")
        report_table.add_row("[bold]–í—Å–µ–≥–æ –∑–∞–¥–∞–Ω–∏–π –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é:", f"[bold bright_magenta]{len(jobs_to_process):,}[/bold bright_magenta]")

        console.print(report_table)

        choices = ["s", "q"]
        prompt_text = "\n[bold]–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: ([green]S[/green])—Ç–∞—Ä—Ç / ([red]Q[/red])uit"
        if malformed_count > 0:
            choices.append("e")
            prompt_text += " / ([yellow]E[/yellow])rrors"

        choice = Prompt.ask(prompt_text, choices=choices, default="s").lower()

        if choice == 's': break
        elif choice == 'q': console.print("[yellow]–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.[/yellow]"); sys.exit(0)
        elif choice == 'e' and malformed_count > 0:
            console.print("\n[bold yellow]----- –°–ø–∏—Å–æ–∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å—Ç—Ä–æ–∫ (–ø–µ—Ä–≤—ã–µ 50) -----[/bold yellow]")
            for num, line, reason in malformed_lines[:50]: console.print(f"[dim]–°—Ç—Ä–æ–∫–∞ #{num} ({reason}):[/dim] {line}")
            console.input("\n[bold]–ù–∞–∂–º–∏—Ç–µ [green]Enter[/green] –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é...[/bold]")
            console.clear()

    plan_summary = {
        "sequences": {"count": len(seq_jobs), "size": sum(j['size'] for j in seq_jobs)},
        "files": {"count": len(file_jobs), "size": sum(j['size'] for j in file_jobs)},
        "total": {"count": len(jobs_to_process), "size": sum(j['size'] for j in jobs_to_process)},
        "skipped": len(jobs) - len(jobs_to_process)
    }
    return jobs_to_process, plan_summary


# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–ª–∏–∫–æ–º
def process_job_worker(job, config, disk_manager):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ –∑–∞–¥–∞–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è –µ–¥–∏–Ω—ã–π –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—Ç–∑—ã–≤—á–∏–≤–æ—Å—Ç—å TUI, –ø–æ–∫–∞–∑—ã–≤–∞—è —Ç–µ–∫—É—â—É—é –∑–∞–¥–∞—á—É.
    """
    thread_id = get_ident()
    is_dry_run = config['dry_run']

    short_name = job.get('tar_filename') or os.path.basename(job['key'])
    status_text = f"[yellow]–ê—Ä—Ö–∏–≤–∏—Ä—É—é:[/] {short_name}" if job['type'] == 'sequence' else f"[cyan]–ö–æ–ø–∏—Ä—É—é:[/] {short_name}"

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –≤–∏–¥–µ–Ω –≤ TUI
    worker_stats[thread_id] = {"status": status_text, "speed": "", "progress": None}

    try:
        dest_mount_point = disk_manager.get_current_destination()
        source_root = config.get('source_root')
        destination_root = config.get('destination_root', '/')
        absolute_source_key = job['key']

        if source_root and absolute_source_key.startswith(os.path.normpath(source_root) + os.sep):
            rel_path = os.path.relpath(absolute_source_key, source_root)
        else:
            rel_path = absolute_source_key.lstrip(os.path.sep)

        dest_path = os.path.normpath(os.path.join(dest_mount_point, destination_root.lstrip(os.path.sep), rel_path))

        if job['type'] == 'sequence':
            # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –æ–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä–∞—è, –ø—Ä–æ—Å—Ç–æ –≤—ã–ø–æ–ª–Ω—è–µ–º
            if not is_dry_run:
                if not archive_sequence_to_destination(job, dest_path):
                    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤ {short_name}")
            else:
                time.sleep(0.05) # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –¥–ª—è dry-run
            source_keys_to_log = job['source_files']

        else: # 'file'
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–∏–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π Popen
            source_keys_to_log = [absolute_source_key]
            if not is_dry_run:
                if not os.path.exists(absolute_source_key):
                    raise FileNotFoundError(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {absolute_source_key}")
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)

                # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —á–µ—Ä–µ–∑ Popen ---
                rsync_cmd = ["rsync", "-a", "--checksum", absolute_source_key, dest_path]

                # –ó–∞–ø—É—Å–∫–∞–µ–º rsync –∏ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è –ø–æ—Ç–æ–∫
                process = subprocess.Popen(rsync_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)

                # –ü—Ä–æ—Å—Ç–æ –∂–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤ —Ü–∏–∫–ª–µ, –ø–æ–∑–≤–æ–ª—è—è TUI –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è
                while process.poll() is None:
                    time.sleep(0.2) # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã –Ω–µ –Ω–∞–≥—Ä—É–∂–∞—Ç—å CPU

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–¥ –≤–æ–∑–≤—Ä–∞—Ç–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                if process.returncode != 0:
                    stderr_output = process.stderr.read().decode('utf-8', errors='ignore')
                    raise subprocess.CalledProcessError(process.returncode, rsync_cmd, stderr=stderr_output)
            else:
                time.sleep(0.05) # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –¥–ª—è dry-run

        worker_stats[thread_id] = {"status": "[green]–°–≤–æ–±–æ–¥–µ–Ω[/green]", "speed": "", "progress": None}
        return source_keys_to_log, dest_path, job['size'], job['type']

    except Exception as e:
        if not isinstance(e, KeyboardInterrupt):
            worker_stats[thread_id] = {"status": f"[red]–û—à–∏–±–∫–∞:[/] {short_name}", "speed": "ERROR", "progress": None}
            log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {job['key']}: {e}")
            with open(config['error_log_file'], "a", encoding='utf-8') as f: f.write(f"{time.asctime()};{job['key']};{e}\n")
        return None, None, 0, job['type']

# --- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ TUI ---

def make_layout() -> Layout:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É TUI."""
    layout = Layout(name="root")
    layout.split_column(
        Layout(name="top", ratio=1),
        Layout(name="middle"),
        Layout(name="bottom", size=3)
    )
    layout["top"].split_row(Layout(name="summary"), Layout(name="disks"))
    return layout

def generate_summary_panel(plan, completed) -> Panel:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞–Ω–µ–ª—å —Å–≤–æ–¥–∫–∏ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º."""
    table = Table(box=None, expand=True)
    table.add_column("–¢–∏–ø –∑–∞–¥–∞–Ω–∏—è", style="cyan", no_wrap=True)
    table.add_column("–í—ã–ø–æ–ª–Ω–µ–Ω–æ (—à—Ç)", style="green", justify="right")
    table.add_column("–†–∞–∑–º–µ—Ä", style="green", justify="right")

    s_done, s_total = completed['sequence']['count'], plan['sequences']['count']
    s_size_done, s_size_total = completed['sequence']['size'], plan['sequences']['size']
    table.add_row("–ê—Ä—Ö–∏–≤–∞—Ü–∏—è —Å–µ–∫–≤–µ–Ω—Ü–∏–π", f"{s_done} / {s_total}", f"{decimal(s_size_done)} / {decimal(s_size_total)}")

    f_done, f_total = completed['files']['count'], plan['files']['count']
    f_size_done, f_size_total = completed['files']['size'], plan['files']['size']
    table.add_row("–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤", f"{f_done} / {f_total}", f"{decimal(f_size_done)} / {decimal(f_size_total)}")

    table.add_row("[bold]–í—Å–µ–≥–æ[/bold]", f"[bold]{s_done + f_done} / {s_total + f_total}[/bold]", f"[bold]{decimal(s_size_done + f_size_done)} / {decimal(s_size_total + f_size_total)}[/bold]")
    return Panel(table, title="üìä –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", border_style="yellow")

def generate_disks_panel(disk_manager: DiskManager, config) -> Panel:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞–Ω–µ–ª—å —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º –¥–∏—Å–∫–æ–≤."""
    table = Table(box=None, expand=True)
    table.add_column("–î–∏—Å–∫", style="white", no_wrap=True)
    table.add_column("–ó–∞–ø–æ–ª–Ω–µ–Ω–æ", style="green", ratio=1)
    table.add_column("%", style="bold", justify="right")
    for mount, percent in disk_manager.get_all_disks_status():
        color = "green" if percent < config['threshold'] else "red"
        bar = Progress(BarColumn(bar_width=None, style=color, complete_style=color), expand=True)
        task_id = bar.add_task("d", total=100); bar.update(task_id, completed=percent)
        is_active = " (*)" if mount == disk_manager.active_disk else ""
        table.add_row(f"[bold]{mount}{is_active}[/bold]", bar, f"{percent:.1f}%")
    return Panel(table, title="üì¶ –î–∏—Å–∫–∏", border_style="blue")

def generate_workers_panel(threads) -> Panel:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞–Ω–µ–ª—å –ø–æ—Ç–æ–∫–æ–≤ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏."""
    table = Table.grid(expand=True)
    table.add_column("–ü–æ—Ç–æ–∫", justify="center", style="cyan", width=12)
    table.add_column("–°—Ç–∞—Ç—É—Å", style="white", no_wrap=True, ratio=2)
    table.add_column("–°–∫–æ—Ä–æ—Å—Ç—å", justify="right", style="magenta", width=15)

    for tid in sorted(worker_stats.keys()):
        stats = worker_stats.get(tid, {})
        status_renderable = stats.get("status", "[grey50]–û–∂–∏–¥–∞–Ω–∏–µ...[/grey50]")
        speed_str = stats.get("speed", "[dim]---[/dim]")
        progress_val = stats.get("progress")

        if progress_val is not None and 0 <= progress_val <= 100:
            p_bar = Progress(BarColumn(bar_width=None), TextColumn("{task.percentage:>3.0f}%"), expand=True)
            p_bar.add_task("p", total=100, completed=progress_val)
            status_grid = Table.grid(expand=True)
            status_grid.add_row(status_renderable)
            status_grid.add_row(p_bar)
            table.add_row(str(tid), status_grid, speed_str)
        else:
            table.add_row(str(tid), status_renderable, speed_str)

    return Panel(table, title=f"üë∑ –ü–æ—Ç–æ–∫–∏ ({threads})", border_style="green")


# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ ---

# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–ª–∏–∫–æ–º
def main(args):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    config = load_config()
    if args.dry_run: config['dry_run'] = True

    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä—Å–∏—é –¥–ª—è —Ñ–∏–∫—Å–∞ ---
    console.rule(f"[bold]Smart Archiver & Copier v2.4.1[/bold] | –†–µ–∂–∏–º: {'Dry Run' if config['dry_run'] else '–†–µ–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞'}")

    is_dry_run = config['dry_run']
    if is_dry_run:
        dry_run_log_path = config['dry_run_mapping_file']
        try:
            with open(dry_run_log_path, 'w', newline='', encoding='utf-8') as f:
                csv.writer(f).writerow(["source_path", "destination_path"])
            console.print(f"–û—Ç—á–µ—Ç dry-run –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: [cyan]{dry_run_log_path}[/cyan]")
        except IOError as e: console.print(f"[bold red]–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –æ—Ç—á–µ—Ç–∞ –¥–ª—è dry-run: {e}[/bold red]")

    load_previous_state(config['state_file'])

    jobs_to_process, plan_summary = analyze_and_plan_jobs(args.input_file, config)
    if not jobs_to_process: return

    if not is_dry_run:
        disk_manager = DiskManager(config['mount_points'], config['threshold'])
        if not disk_manager.active_disk: return
    else:
        class FakeDiskManager:
            def __init__(self, mount_points):
                self.mount_points = mount_points
                self.active_disk = mount_points[0] if mount_points else "/dry/run/dest"
            def get_current_destination(self): return self.active_disk
            def get_all_disks_status(self): return [(p, 0.0) for p in self.mount_points]
        disk_manager = FakeDiskManager(config['mount_points'])
        log.info("Dry-run: —Å–∏–º—É–ª—è—Ü–∏—è –∑–∞–ø–∏—Å–∏ –Ω–∞ –¥–∏—Å–∫.")

    console.rule("[yellow]–®–∞–≥ 2: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ[/]")
    time.sleep(1)

    job_counter_column = TextColumn(f"[cyan]0/{plan_summary['total']['count']} –∑–∞–¥–∞–Ω–∏–π[/cyan]")
    progress = Progress(TextColumn("[bold blue]–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å:[/bold blue]"), BarColumn(), TaskProgressColumn(), TextColumn("‚Ä¢"),
                        job_counter_column, TextColumn("‚Ä¢"), TransferSpeedColumn(), TextColumn("‚Ä¢"), TimeRemainingColumn())
    main_task = progress.add_task("–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ", total=plan_summary['total']['count'])
    layout = make_layout()
    layout["bottom"].update(Panel(progress, title="üöÄ –ü—Ä–æ—Ü–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", border_style="magenta", expand=False))

    completed_stats = {"sequence": {"count": 0, "size": 0}, "files": {"count": 0, "size": 0}}
    jobs_completed_count, all_jobs_successful = 0, True

    state_log = config['state_file']
    mapping_log = config['dry_run_mapping_file'] if is_dry_run else config['mapping_file']

    try:
        with Live(layout, screen=True, redirect_stderr=False, vertical_overflow="visible", refresh_per_second=4) as live:
            with ThreadPoolExecutor(max_workers=config['threads']) as executor:
                layout["summary"].update(generate_summary_panel(plan_summary, completed_stats))
                layout["disks"].update(generate_disks_panel(disk_manager, config))
                layout["middle"].update(generate_workers_panel(config['threads']))

                future_to_job = {executor.submit(process_job_worker, job, config, disk_manager): job for job in jobs_to_process}

                # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–æ—Å—Ç–æ–º—É –∏ –Ω–∞–¥–µ–∂–Ω–æ–º—É —Ü–∏–∫–ª—É for ---
                for future in as_completed(future_to_job):
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞–Ω–µ–ª–∏ –≤ –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–∑—ã–≤—á–∏–≤–æ—Å—Ç–∏
                    layout["middle"].update(generate_workers_panel(config['threads']))

                    source_keys, dest_path, size_processed, job_type = future.result()

                    if source_keys is not None:
                        for key in source_keys: write_log(state_log, mapping_log, key, dest_path, is_dry_run=is_dry_run)

                        if job_type == 'sequence':
                            completed_stats['sequence']['count'] += 1
                            completed_stats['sequence']['size'] += size_processed
                        else:
                            completed_stats['files']['count'] += 1
                            completed_stats['files']['size'] += size_processed
                    else:
                        all_jobs_successful = False

                    jobs_completed_count += 1
                    progress.update(main_task, advance=1)
                    job_counter_column.text_format = f"[cyan]{jobs_completed_count}/{plan_summary['total']['count']} –∑–∞–¥–∞–Ω–∏–π[/cyan]"

                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞–Ω–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    layout["summary"].update(generate_summary_panel(plan_summary, completed_stats))
                    if not is_dry_run:
                        layout["disks"].update(generate_disks_panel(disk_manager, config))

    except (KeyboardInterrupt, SystemExit):
        console.print("\n[bold red]–ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω. –í —Ä–µ–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ.[/bold red]")
        sys.exit(1)

    if all_jobs_successful and progress.finished:
        console.rule("[bold green]‚úÖ –í—Å–µ –∑–∞–¥–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã[/bold green]")
    else:
        console.rule("[bold yellow]–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –Ω–æ –±—ã–ª–∏ –æ—à–∏–±–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥.[/bold yellow]")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç CSV, –∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –∏ –∫–æ–ø–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.",
        prog="copeer"
    )
    parser.add_argument(
        "-v", "--version",
        action="version",
        version=f"%(prog)s v{__version__}"
    )
    parser.add_argument("input_file", help="–ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.")
    parser.add_argument("--dry-run", action="store_true", help="–í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è.")
    # –£–±–∏—Ä–∞–µ–º source_base, —Ç.–∫. —Ç–µ–ø–µ—Ä—å –≤—Å–µ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    # parser.add_argument("--source-base", help="–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π –∏–∑ CSV.")
    args = parser.parse_args()
    main(args)
